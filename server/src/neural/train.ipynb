{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboostNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading xgboost-2.0.2-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from xgboost) (1.24.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from xgboost) (1.11.3)\n",
      "Downloading xgboost-2.0.2-py3-none-win_amd64.whl (99.8 MB)\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.8 MB 435.7 kB/s eta 0:03:49\n",
      "   ---------------------------------------- 0.1/99.8 MB 365.7 kB/s eta 0:04:33\n",
      "   ---------------------------------------- 0.1/99.8 MB 595.3 kB/s eta 0:02:48\n",
      "   ---------------------------------------- 0.1/99.8 MB 568.9 kB/s eta 0:02:56\n",
      "   ---------------------------------------- 0.2/99.8 MB 811.5 kB/s eta 0:02:03\n",
      "   ---------------------------------------- 0.3/99.8 MB 896.4 kB/s eta 0:01:51\n",
      "   ---------------------------------------- 0.4/99.8 MB 1.1 MB/s eta 0:01:35\n",
      "   ---------------------------------------- 0.6/99.8 MB 1.4 MB/s eta 0:01:09\n",
      "   ---------------------------------------- 0.7/99.8 MB 1.6 MB/s eta 0:01:01\n",
      "   ---------------------------------------- 1.2/99.8 MB 2.3 MB/s eta 0:00:43\n",
      "    --------------------------------------- 1.3/99.8 MB 2.4 MB/s eta 0:00:41\n",
      "    --------------------------------------- 1.8/99.8 MB 3.0 MB/s eta 0:00:33\n",
      "    --------------------------------------- 2.3/99.8 MB 3.6 MB/s eta 0:00:27\n",
      "   - -------------------------------------- 2.9/99.8 MB 4.2 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 3.4/99.8 MB 4.6 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 4.0/99.8 MB 5.1 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 4.0/99.8 MB 5.0 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 4.0/99.8 MB 5.0 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 4.1/99.8 MB 4.3 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 4.8/99.8 MB 4.9 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 5.4/99.8 MB 5.3 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 6.4/99.8 MB 5.9 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 7.5/99.8 MB 6.7 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 7.5/99.8 MB 6.7 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 7.5/99.8 MB 6.7 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 8.8/99.8 MB 6.8 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 10.0/99.8 MB 7.5 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 10.4/99.8 MB 9.2 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 11.0/99.8 MB 10.7 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 11.6/99.8 MB 11.1 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 12.1/99.8 MB 11.3 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 12.6/99.8 MB 11.1 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 13.1/99.8 MB 10.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 13.5/99.8 MB 10.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.0/99.8 MB 10.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.5/99.8 MB 13.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 15.0/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 15.5/99.8 MB 12.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 15.9/99.8 MB 12.4 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 16.4/99.8 MB 12.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 16.9/99.8 MB 11.7 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 17.4/99.8 MB 11.3 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 18.0/99.8 MB 12.8 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 18.5/99.8 MB 12.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 19.0/99.8 MB 11.9 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 19.6/99.8 MB 11.3 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 20.2/99.8 MB 11.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 20.7/99.8 MB 11.3 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 21.3/99.8 MB 11.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 21.8/99.8 MB 11.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 22.4/99.8 MB 11.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 23.0/99.8 MB 11.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 23.5/99.8 MB 11.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 24.1/99.8 MB 11.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 24.6/99.8 MB 11.1 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 25.2/99.8 MB 11.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 25.7/99.8 MB 11.1 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 26.2/99.8 MB 11.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 26.7/99.8 MB 11.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 27.3/99.8 MB 11.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 27.8/99.8 MB 11.3 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 28.3/99.8 MB 11.3 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 28.8/99.8 MB 11.3 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 29.3/99.8 MB 11.3 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 29.8/99.8 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 30.2/99.8 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 30.7/99.8 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 31.2/99.8 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 31.8/99.8 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 32.2/99.8 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 32.7/99.8 MB 11.3 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 33.2/99.8 MB 11.3 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 33.8/99.8 MB 11.1 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 34.3/99.8 MB 11.3 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 34.8/99.8 MB 11.1 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 35.3/99.8 MB 11.1 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 35.8/99.8 MB 11.1 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 36.4/99.8 MB 11.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 36.9/99.8 MB 11.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 37.2/99.8 MB 10.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 37.2/99.8 MB 10.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 37.2/99.8 MB 10.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 37.2/99.8 MB 10.9 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 37.7/99.8 MB 9.9 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 38.3/99.8 MB 9.6 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 39.5/99.8 MB 10.4 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 39.8/99.8 MB 10.1 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 40.3/99.8 MB 10.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 40.7/99.8 MB 9.9 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 41.2/99.8 MB 9.8 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 41.5/99.8 MB 9.6 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 42.0/99.8 MB 9.8 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 42.5/99.8 MB 9.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 42.9/99.8 MB 9.5 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 43.4/99.8 MB 9.5 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 43.7/99.8 MB 9.2 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 44.3/99.8 MB 9.2 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 44.6/99.8 MB 9.1 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 45.0/99.8 MB 9.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 45.5/99.8 MB 9.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 45.8/99.8 MB 8.8 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 46.4/99.8 MB 8.8 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 46.7/99.8 MB 8.7 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 47.1/99.8 MB 8.7 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 47.5/99.8 MB 9.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 47.9/99.8 MB 9.6 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 48.3/99.8 MB 9.6 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 48.8/99.8 MB 9.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 49.2/99.8 MB 9.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 49.5/99.8 MB 9.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 49.9/99.8 MB 9.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 50.3/99.8 MB 9.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 50.8/99.8 MB 9.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 51.3/99.8 MB 9.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 51.6/99.8 MB 9.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 52.0/99.8 MB 9.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 52.4/99.8 MB 9.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 52.8/99.8 MB 9.2 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 53.2/99.8 MB 9.2 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 53.6/99.8 MB 9.2 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 54.0/99.8 MB 9.4 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 54.5/99.8 MB 9.4 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 54.9/99.8 MB 9.4 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 55.3/99.8 MB 9.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 55.7/99.8 MB 9.4 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 56.2/99.8 MB 9.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 56.7/99.8 MB 9.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 57.1/99.8 MB 9.5 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 57.6/99.8 MB 9.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 58.0/99.8 MB 9.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 58.4/99.8 MB 9.8 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 58.8/99.8 MB 9.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 59.2/99.8 MB 9.8 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 59.7/99.8 MB 9.8 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 60.2/99.8 MB 9.8 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 60.5/99.8 MB 9.8 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 60.9/99.8 MB 9.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 61.5/99.8 MB 9.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 62.0/99.8 MB 9.9 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 62.4/99.8 MB 9.9 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 62.8/99.8 MB 9.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 63.2/99.8 MB 9.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 63.7/99.8 MB 9.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 64.2/99.8 MB 9.9 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 64.6/99.8 MB 9.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 65.1/99.8 MB 9.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 65.5/99.8 MB 9.8 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 65.9/99.8 MB 9.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 66.4/99.8 MB 9.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 66.8/99.8 MB 9.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 67.3/99.8 MB 10.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 67.7/99.8 MB 9.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 68.2/99.8 MB 9.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 68.5/99.8 MB 9.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 69.0/99.8 MB 10.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 69.4/99.8 MB 10.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 70.0/99.8 MB 10.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 70.5/99.8 MB 9.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 70.9/99.8 MB 9.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 71.5/99.8 MB 10.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 72.0/99.8 MB 10.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 72.4/99.8 MB 10.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 72.9/99.8 MB 10.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 73.4/99.8 MB 10.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 74.0/99.8 MB 10.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 74.5/99.8 MB 10.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 75.0/99.8 MB 9.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 75.5/99.8 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 75.9/99.8 MB 10.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 76.4/99.8 MB 10.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 76.9/99.8 MB 10.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 77.3/99.8 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 77.8/99.8 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 78.2/99.8 MB 10.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 78.8/99.8 MB 10.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 79.2/99.8 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 79.6/99.8 MB 10.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.1/99.8 MB 10.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.6/99.8 MB 10.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 81.0/99.8 MB 10.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 81.5/99.8 MB 10.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 82.0/99.8 MB 10.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 82.5/99.8 MB 10.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 83.0/99.8 MB 10.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 83.5/99.8 MB 10.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 83.9/99.8 MB 10.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.2/99.8 MB 10.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.5/99.8 MB 9.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.5/99.8 MB 9.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.5/99.8 MB 9.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.8/99.8 MB 9.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 85.9/99.8 MB 9.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 86.3/99.8 MB 9.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 86.6/99.8 MB 9.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 87.0/99.8 MB 9.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 87.4/99.8 MB 9.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 87.6/99.8 MB 8.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.0/99.8 MB 9.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.3/99.8 MB 8.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.8/99.8 MB 8.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 89.1/99.8 MB 8.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 89.4/99.8 MB 8.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 89.9/99.8 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 90.3/99.8 MB 8.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 90.8/99.8 MB 8.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.0/99.8 MB 8.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.4/99.8 MB 8.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.7/99.8 MB 7.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 92.1/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 92.4/99.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 92.7/99.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.1/99.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.4/99.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.8/99.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.3/99.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.6/99.8 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.0/99.8 MB 8.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.3/99.8 MB 8.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.7/99.8 MB 7.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.9/99.8 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.4/99.8 MB 7.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.8/99.8 MB 7.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 97.1/99.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.5/99.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.8/99.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.3/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.6/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.9/99.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.4/99.8 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 99.8/99.8 MB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.2\n",
      "Collecting catboostNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading catboost-1.2.2-cp310-cp310-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "     ---------------------------------------- 0.0/47.0 kB ? eta -:--:--\n",
      "     ------------------------ ------------- 30.7/47.0 kB 660.6 kB/s eta 0:00:01\n",
      "     ------------------------ ------------- 30.7/47.0 kB 660.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 47.0/47.0 kB 262.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from catboost) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from catboost) (1.24.2)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from catboost) (2.1.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from catboost) (1.11.3)\n",
      "Collecting plotly (from catboost)\n",
      "  Downloading plotly-5.18.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: six in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas>=0.24->catboost) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->catboost) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->catboost) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->catboost) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->catboost) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->catboost) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\arbol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->catboost) (3.1.1)\n",
      "Collecting tenacity>=6.2.0 (from plotly->catboost)\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading catboost-1.2.2-cp310-cp310-win_amd64.whl (101.0 MB)\n",
      "   ---------------------------------------- 0.0/101.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/101.0 MB 1.7 MB/s eta 0:01:02\n",
      "   ---------------------------------------- 0.1/101.0 MB 1.3 MB/s eta 0:01:18\n",
      "   ---------------------------------------- 0.1/101.0 MB 901.1 kB/s eta 0:01:52\n",
      "   ---------------------------------------- 0.3/101.0 MB 1.6 MB/s eta 0:01:04\n",
      "   ---------------------------------------- 0.3/101.0 MB 1.3 MB/s eta 0:01:17\n",
      "   ---------------------------------------- 0.5/101.0 MB 1.8 MB/s eta 0:00:58\n",
      "   ---------------------------------------- 0.6/101.0 MB 2.2 MB/s eta 0:00:47\n",
      "   ---------------------------------------- 0.8/101.0 MB 2.4 MB/s eta 0:00:43\n",
      "    --------------------------------------- 1.3/101.0 MB 3.3 MB/s eta 0:00:30\n",
      "    --------------------------------------- 1.5/101.0 MB 3.4 MB/s eta 0:00:29\n",
      "    --------------------------------------- 2.1/101.0 MB 4.3 MB/s eta 0:00:24\n",
      "    --------------------------------------- 2.5/101.0 MB 4.7 MB/s eta 0:00:22\n",
      "    --------------------------------------- 2.5/101.0 MB 4.7 MB/s eta 0:00:22\n",
      "    --------------------------------------- 2.5/101.0 MB 4.7 MB/s eta 0:00:22\n",
      "    --------------------------------------- 2.5/101.0 MB 4.7 MB/s eta 0:00:22\n",
      "    --------------------------------------- 2.5/101.0 MB 4.7 MB/s eta 0:00:22\n",
      "    --------------------------------------- 2.5/101.0 MB 4.7 MB/s eta 0:00:22\n",
      "   - -------------------------------------- 3.4/101.0 MB 4.0 MB/s eta 0:00:25\n",
      "   -- ------------------------------------- 5.6/101.0 MB 6.5 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 5.7/101.0 MB 6.2 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 6.2/101.0 MB 6.4 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 6.8/101.0 MB 6.7 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 7.3/101.0 MB 6.9 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 7.9/101.0 MB 7.0 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 8.2/101.0 MB 7.2 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 8.2/101.0 MB 7.2 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 8.2/101.0 MB 7.2 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 8.3/101.0 MB 6.5 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 10.3/101.0 MB 8.3 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 10.8/101.0 MB 9.4 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 11.3/101.0 MB 9.6 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 11.7/101.0 MB 9.9 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 12.2/101.0 MB 9.9 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 12.7/101.0 MB 9.9 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 13.2/101.0 MB 13.1 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 13.6/101.0 MB 12.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.1/101.0 MB 11.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.5/101.0 MB 11.3 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 15.0/101.0 MB 11.1 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 15.4/101.0 MB 10.6 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 15.9/101.0 MB 10.9 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 16.4/101.0 MB 11.1 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 16.9/101.0 MB 11.1 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 17.3/101.0 MB 10.9 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 17.8/101.0 MB 10.7 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 18.4/101.0 MB 10.9 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 18.9/101.0 MB 12.1 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 19.3/101.0 MB 11.5 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 19.9/101.0 MB 11.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 20.4/101.0 MB 10.7 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 21.0/101.0 MB 10.7 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 21.4/101.0 MB 10.9 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 22.0/101.0 MB 10.7 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 22.6/101.0 MB 10.7 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 23.0/101.0 MB 10.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 23.5/101.0 MB 10.7 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 24.1/101.0 MB 10.7 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 24.6/101.0 MB 10.7 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 25.2/101.0 MB 10.7 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 25.7/101.0 MB 10.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 26.2/101.0 MB 10.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 26.7/101.0 MB 10.7 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 27.2/101.0 MB 10.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 27.7/101.0 MB 10.9 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 28.2/101.0 MB 10.9 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 28.6/101.0 MB 10.9 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 29.1/101.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 29.6/101.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 30.1/101.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 30.6/101.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 31.0/101.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 31.5/101.0 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 32.1/101.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 32.6/101.0 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 33.1/101.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 33.6/101.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 34.1/101.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 34.7/101.0 MB 11.3 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 35.2/101.0 MB 11.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 35.7/101.0 MB 11.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 36.1/101.0 MB 11.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 36.7/101.0 MB 11.1 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 37.2/101.0 MB 11.1 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 37.8/101.0 MB 11.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 38.2/101.0 MB 11.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 38.8/101.0 MB 11.1 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 39.2/101.0 MB 11.1 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 39.7/101.0 MB 11.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 40.2/101.0 MB 11.3 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 40.7/101.0 MB 11.3 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 41.2/101.0 MB 10.9 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 41.7/101.0 MB 10.7 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 42.3/101.0 MB 11.1 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 42.9/101.0 MB 11.1 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 43.4/101.0 MB 10.9 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 43.8/101.0 MB 11.1 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 44.3/101.0 MB 10.9 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 44.7/101.0 MB 10.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 45.2/101.0 MB 10.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 45.3/101.0 MB 10.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 45.3/101.0 MB 10.6 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 45.8/101.0 MB 9.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 46.3/101.0 MB 9.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 47.6/101.0 MB 9.9 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 47.8/101.0 MB 9.6 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 48.2/101.0 MB 9.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 48.7/101.0 MB 9.4 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 49.2/101.0 MB 9.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 49.6/101.0 MB 9.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 50.0/101.0 MB 9.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 50.3/101.0 MB 9.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 50.7/101.0 MB 9.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 51.1/101.0 MB 9.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 51.6/101.0 MB 9.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 51.9/101.0 MB 8.8 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 52.4/101.0 MB 8.8 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 52.8/101.0 MB 8.8 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 53.3/101.0 MB 8.7 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 53.6/101.0 MB 8.6 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 53.7/101.0 MB 8.4 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 53.9/101.0 MB 8.2 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 54.7/101.0 MB 8.4 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 54.8/101.0 MB 8.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 55.3/101.0 MB 8.1 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 55.7/101.0 MB 9.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 56.0/101.0 MB 9.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 56.6/101.0 MB 8.8 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 56.9/101.0 MB 8.7 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 57.5/101.0 MB 8.4 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 57.9/101.0 MB 8.4 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 58.4/101.0 MB 8.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 58.8/101.0 MB 8.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 59.1/101.0 MB 8.5 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 59.6/101.0 MB 8.7 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 60.0/101.0 MB 8.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 60.4/101.0 MB 8.7 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 60.9/101.0 MB 8.7 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 61.2/101.0 MB 8.7 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 61.7/101.0 MB 8.7 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 62.1/101.0 MB 8.8 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 62.5/101.0 MB 8.7 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 63.0/101.0 MB 8.7 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 63.5/101.0 MB 9.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 64.0/101.0 MB 9.2 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 64.5/101.0 MB 9.1 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 64.9/101.0 MB 9.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 65.5/101.0 MB 9.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 65.9/101.0 MB 9.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 66.5/101.0 MB 9.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 67.1/101.0 MB 9.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 67.6/101.0 MB 9.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 68.1/101.0 MB 9.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 68.6/101.0 MB 9.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 69.1/101.0 MB 9.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 69.6/101.0 MB 9.6 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 70.0/101.0 MB 9.6 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 70.4/101.0 MB 9.5 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 70.9/101.0 MB 9.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 71.3/101.0 MB 9.5 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 71.8/101.0 MB 9.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 72.3/101.0 MB 9.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 72.7/101.0 MB 9.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 73.3/101.0 MB 9.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 73.7/101.0 MB 9.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 74.3/101.0 MB 9.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 74.9/101.0 MB 9.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 75.6/101.0 MB 9.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 75.9/101.0 MB 9.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 76.2/101.0 MB 9.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 76.9/101.0 MB 9.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 77.3/101.0 MB 9.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 77.8/101.0 MB 9.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 78.3/101.0 MB 9.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 78.7/101.0 MB 9.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 79.3/101.0 MB 9.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 79.8/101.0 MB 9.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 80.3/101.0 MB 9.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 80.8/101.0 MB 9.6 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 81.2/101.0 MB 9.6 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 81.8/101.0 MB 9.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 82.3/101.0 MB 9.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 82.8/101.0 MB 9.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 83.3/101.0 MB 9.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 83.8/101.0 MB 9.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.3/101.0 MB 9.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.9/101.0 MB 9.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 85.4/101.0 MB 10.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 85.9/101.0 MB 9.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 86.3/101.0 MB 9.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 86.9/101.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 87.4/101.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 87.9/101.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.5/101.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.9/101.0 MB 10.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 89.4/101.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 89.9/101.0 MB 10.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 90.5/101.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 90.9/101.0 MB 10.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.5/101.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 92.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 92.5/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 93.1/101.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.7/101.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.3/101.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.7/101.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 95.2/101.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 95.7/101.0 MB 10.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.2/101.0 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.9/101.0 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 97.5/101.0 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 98.1/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 98.4/101.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.1/101.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.6/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  100.3/101.0 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  100.8/101.0 MB 10.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.0/101.0 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading plotly-5.18.0-py3-none-any.whl (15.6 MB)\n",
      "   ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/15.6 MB 27.5 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 1.0/15.6 MB 15.1 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 1.3/15.6 MB 12.0 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.8/15.6 MB 12.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 2.2/15.6 MB 11.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.7/15.6 MB 11.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 3.1/15.6 MB 11.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.6/15.6 MB 11.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.1/15.6 MB 11.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.5/15.6 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.9/15.6 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 5.4/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 5.9/15.6 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 6.4/15.6 MB 10.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 6.9/15.6 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.4/15.6 MB 10.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 7.9/15.6 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.4/15.6 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 8.9/15.6 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.4/15.6 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.8/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.4/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.8/15.6 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.3/15.6 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.8/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.3/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.8/15.6 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.3/15.6 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.8/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.2/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.7/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.1/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.6/15.6 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, graphviz, plotly, catboost\n",
      "Successfully installed catboost-1.2.2 graphviz-0.20.1 plotly-5.18.0 tenacity-8.2.3\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost\n",
    "%pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ,    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import validation_curve, train_test_split\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "from keras import backend as K\n",
    "import joblib\n",
    "\n",
    "dbname = 'football'\n",
    "user = 'postgres'\n",
    "password = 'TheDarkKhight'\n",
    "host = 'localhost'\n",
    "port = 5432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host, port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "def process_row(row):\n",
    "    fixed_string = re.sub(r'\\}\\{', ',', all_elo[all_elo['id'] == row['team_id']]['prev_elos'].iloc[0])\n",
    "    fixed_string = re.sub(r'(\\d+)(?=:)', r'\"\\1\"', fixed_string)\n",
    "    json_string = json.loads(fixed_string)\n",
    "    return json_string[f\"{int(row['time'])}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_elo(frame):\n",
    "    frame['elo'] = frame.apply(process_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixtueID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>home_team_id</th>\n",
       "      <th>home_team_name</th>\n",
       "      <th>home_goals</th>\n",
       "      <th>away_goals</th>\n",
       "      <th>away_team_id</th>\n",
       "      <th>away_team_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1035037</td>\n",
       "      <td>1691780400</td>\n",
       "      <td>44</td>\n",
       "      <td>Burnley</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>Manchester City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1035038</td>\n",
       "      <td>1691839800</td>\n",
       "      <td>42</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>Nottingham Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1035039</td>\n",
       "      <td>1691848800</td>\n",
       "      <td>35</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>West Ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1035040</td>\n",
       "      <td>1691848800</td>\n",
       "      <td>51</td>\n",
       "      <td>Brighton</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1359</td>\n",
       "      <td>Luton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1035041</td>\n",
       "      <td>1691848800</td>\n",
       "      <td>45</td>\n",
       "      <td>Everton</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>Fulham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1035300</td>\n",
       "      <td>1700924400</td>\n",
       "      <td>34</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>Chelsea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1035294</td>\n",
       "      <td>1700933400</td>\n",
       "      <td>55</td>\n",
       "      <td>Brentford</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>Arsenal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1035303</td>\n",
       "      <td>1701007200</td>\n",
       "      <td>47</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>Aston Villa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1035296</td>\n",
       "      <td>1701016200</td>\n",
       "      <td>45</td>\n",
       "      <td>Everton</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>Manchester United</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1035297</td>\n",
       "      <td>1701115200</td>\n",
       "      <td>36</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>Wolves</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fixtueID   timestamp  home_team_id home_team_name  home_goals  \\\n",
       "0     1035037  1691780400            44        Burnley           0   \n",
       "1     1035038  1691839800            42        Arsenal           2   \n",
       "2     1035039  1691848800            35    Bournemouth           1   \n",
       "3     1035040  1691848800            51       Brighton           4   \n",
       "4     1035041  1691848800            45        Everton           0   \n",
       "..        ...         ...           ...            ...         ...   \n",
       "136   1035300  1700924400            34      Newcastle           4   \n",
       "137   1035294  1700933400            55      Brentford           0   \n",
       "138   1035303  1701007200            47      Tottenham           1   \n",
       "139   1035296  1701016200            45        Everton           0   \n",
       "140   1035297  1701115200            36         Fulham           3   \n",
       "\n",
       "     away_goals  away_team_id     away_team_name  \n",
       "0             3            50    Manchester City  \n",
       "1             1            65  Nottingham Forest  \n",
       "2             1            48           West Ham  \n",
       "3             1          1359              Luton  \n",
       "4             1            36             Fulham  \n",
       "..          ...           ...                ...  \n",
       "136           1            49            Chelsea  \n",
       "137           1            42            Arsenal  \n",
       "138           2            66        Aston Villa  \n",
       "139           3            33  Manchester United  \n",
       "140           2            39             Wolves  \n",
       "\n",
       "[141 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_PL23 = pd.read_csv('elo/csv/England/2023/PL.csv')\n",
    "data_kubokPL23 = pd.read_csv('elo/csv/England/2023/cup.csv')\n",
    "data_kubokPL23 = data_kubokPL23[data_kubokPL23['home_team_id'].isin(data_PL23['home_team_id'].unique()) & data_kubokPL23['away_team_id'].isin(\n",
    "                 data_PL23['home_team_id'].unique())]\n",
    "data_carabaoPL23 = pd.read_csv('elo/csv/England/2023/carabao.csv')\n",
    "data_carabaoPL23 = data_carabaoPL23[data_carabaoPL23['home_team_id'].isin(data_PL23['home_team_id'].unique()) & data_carabaoPL23['away_team_id'].isin(\n",
    "                 data_PL23['home_team_id'].unique())]\n",
    "data_PL23 = data_PL23.merge(data_kubokPL23, how='outer')\n",
    "data_PL23 = data_PL23.merge(data_carabaoPL23, how='outer')\n",
    "data_PL23.sort_values('timestamp', inplace=True)\n",
    "data_PL23.reset_index(inplace=True, drop=True)\n",
    "data_PL23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_RFPL23 = pd.read_csv('elo/csv/Russia/2023/RFPL.csv')\n",
    "data_kubokRFPL23 = pd.read_csv('elo/csv/Russia/2023/KUBOK.csv')\n",
    "data_kubokRFPL23 = data_kubokRFPL23[data_kubokRFPL23['home_team_id'].isin(data_RFPL23['home_team_id'].unique()) & data_kubokRFPL23['away_team_id'].isin(\n",
    "                 data_RFPL23['home_team_id'].unique())]\n",
    "data_RFPL23 = data_RFPL23.merge(data_kubokRFPL23, how='outer').sort_values('timestamp').reset_index(drop=True)\n",
    "data_RFPL23.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_BUNDES23 = pd.read_csv('elo/csv/Germany/2023/Bundes.csv')\n",
    "data_kubokBUNDES23 = pd.read_csv('elo/csv/Germany/2023/DFB.csv')\n",
    "data_kubokBUNDES23 = data_kubokBUNDES23[data_kubokBUNDES23['home_team_id'].isin(data_BUNDES23['home_team_id'].unique()) & data_kubokBUNDES23['away_team_id'].isin(\n",
    "                 data_BUNDES23['home_team_id'].unique())]\n",
    "data_BUNDES23 = data_BUNDES23.merge(data_kubokBUNDES23, how='outer').sort_values('timestamp').reset_index(drop=True)\n",
    "data_BUNDES23.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_SERIE23 = pd.read_csv('elo/csv/Italy/2023/SeriaA.csv')\n",
    "data_kubokSERIE23 = pd.read_csv('elo/csv/Italy/2023/KUBOK.csv')\n",
    "data_kubokSERIE23 = data_kubokSERIE23[data_kubokSERIE23['home_team_id'].isin(data_SERIE23['home_team_id'].unique()) & data_kubokSERIE23['away_team_id'].isin(\n",
    "                 data_SERIE23['home_team_id'].unique())]\n",
    "data_SERIE23 = data_SERIE23.merge(data_kubokSERIE23, how='outer').sort_values('timestamp').reset_index(drop=True)\n",
    "data_SERIE23.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = data_PL23.merge(data_RFPL23, how='outer').sort_values('timestamp').reset_index(drop=True)\n",
    "all_data = all_data.merge(data_BUNDES23, how='outer').sort_values('timestamp').reset_index(drop=True)\n",
    "all_data = all_data.merge(data_SERIE23, how='outer').sort_values('timestamp').reset_index(drop=True)\n",
    "# 22 \n",
    "data_PL22 = pd.read_csv('elo/csv/England/2022/PL.csv')\n",
    "data_kubokPL22 = pd.read_csv('elo/csv/England/2022/cup.csv')\n",
    "data_kubokPL22\n",
    "data_kubokPL22 = data_kubokPL22[data_kubokPL22['home_team_id'].isin(data_PL22['home_team_id'].unique()) & data_kubokPL22['away_team_id'].isin(\n",
    "                 data_PL22['home_team_id'].unique())]\n",
    "data_carabaoPL22 = pd.read_csv('elo/csv/England/2022/carabao.csv')\n",
    "data_carabaoPL22\n",
    "data_carabaoPL22 = data_carabaoPL22[data_carabaoPL22['home_team_id'].isin(data_PL22['home_team_id'].unique()) & data_carabaoPL22['away_team_id'].isin(\n",
    "                 data_PL22['home_team_id'].unique())]\n",
    "data_PL22 = data_PL22.merge(data_kubokPL22, how='outer')\n",
    "data_PL22 = data_PL22.merge(data_carabaoPL22, how='outer')\n",
    "data_PL22.sort_values('timestamp', inplace=True)\n",
    "data_PL22.reset_index(inplace=True, drop=True)\n",
    "\n",
    "data_RFPL22 = pd.read_csv('elo/csv/Russia/2022/RFPL.csv')\n",
    "data_kubokRFPL22 = pd.read_csv('elo/csv/Russia/2022/KUBOK.csv')\n",
    "data_kubokRFPL22 = data_kubokRFPL22[data_kubokRFPL22['home_team_id'].isin(data_RFPL22['home_team_id'].unique()) & data_kubokRFPL22['away_team_id'].isin(\n",
    "                 data_RFPL22['home_team_id'].unique())]\n",
    "data_RFPL22 = data_RFPL22.merge(data_kubokRFPL22, how='outer').sort_values('timestamp').reset_index(drop=True)\n",
    "data_RFPL22.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_BUNDES22 = pd.read_csv('elo/csv/Germany/2022/Bundes.csv')\n",
    "data_kubokBUNDES22 = pd.read_csv('elo/csv/Germany/2022/DFB.csv')\n",
    "data_kubokBUNDES22 = data_kubokBUNDES22[data_kubokBUNDES22['home_team_id'].isin(data_BUNDES22['home_team_id'].unique()) & data_kubokBUNDES22['away_team_id'].isin(\n",
    "                 data_BUNDES22['home_team_id'].unique())]\n",
    "data_BUNDES22 = data_BUNDES22.merge(data_kubokBUNDES22, how='outer').sort_values('timestamp').reset_index(drop=True)\n",
    "data_BUNDES22.reset_index(inplace=True, drop=True)\n",
    "\n",
    "data_SERIE22 = pd.read_csv('elo/csv/Italy/2022/SeriaA.csv')\n",
    "data_kubokSERIE22 = pd.read_csv('elo/csv/Italy/2022/KUBOK.csv')\n",
    "data_kubokSERIE22 = data_kubokSERIE22[data_kubokSERIE22['home_team_id'].isin(data_SERIE22['home_team_id'].unique()) & data_kubokSERIE22['away_team_id'].isin(\n",
    "                 data_SERIE22['home_team_id'].unique())]\n",
    "data_SERIE22 = data_SERIE22.merge(data_kubokSERIE22, how='outer').sort_values('timestamp').reset_index(drop=True)\n",
    "data_SERIE22.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixtueID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>home_team_id</th>\n",
       "      <th>home_team_name</th>\n",
       "      <th>home_goals</th>\n",
       "      <th>away_goals</th>\n",
       "      <th>away_team_id</th>\n",
       "      <th>away_team_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>882140</td>\n",
       "      <td>1685278800</td>\n",
       "      <td>500</td>\n",
       "      <td>Bologna</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>492</td>\n",
       "      <td>Napoli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>882149</td>\n",
       "      <td>1685278800</td>\n",
       "      <td>1579</td>\n",
       "      <td>Monza</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>867</td>\n",
       "      <td>Lecce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>882147</td>\n",
       "      <td>1685289600</td>\n",
       "      <td>487</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>520</td>\n",
       "      <td>Cremonese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>882142</td>\n",
       "      <td>1685299500</td>\n",
       "      <td>496</td>\n",
       "      <td>Juventus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>489</td>\n",
       "      <td>AC Milan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>882156</td>\n",
       "      <td>1685730600</td>\n",
       "      <td>488</td>\n",
       "      <td>Sassuolo</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>502</td>\n",
       "      <td>Fiorentina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>882158</td>\n",
       "      <td>1685809800</td>\n",
       "      <td>503</td>\n",
       "      <td>Torino</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>505</td>\n",
       "      <td>Inter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>882152</td>\n",
       "      <td>1685818800</td>\n",
       "      <td>511</td>\n",
       "      <td>Empoli</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>487</td>\n",
       "      <td>Lazio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>882151</td>\n",
       "      <td>1685818800</td>\n",
       "      <td>520</td>\n",
       "      <td>Cremonese</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>514</td>\n",
       "      <td>Salernitana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>882153</td>\n",
       "      <td>1685896200</td>\n",
       "      <td>492</td>\n",
       "      <td>Napoli</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>498</td>\n",
       "      <td>Sampdoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>882157</td>\n",
       "      <td>1685905200</td>\n",
       "      <td>489</td>\n",
       "      <td>AC Milan</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>504</td>\n",
       "      <td>Verona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>882154</td>\n",
       "      <td>1685905200</td>\n",
       "      <td>497</td>\n",
       "      <td>AS Roma</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>515</td>\n",
       "      <td>Spezia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>882159</td>\n",
       "      <td>1685905200</td>\n",
       "      <td>494</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>496</td>\n",
       "      <td>Juventus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>882150</td>\n",
       "      <td>1685905200</td>\n",
       "      <td>499</td>\n",
       "      <td>Atalanta</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1579</td>\n",
       "      <td>Monza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>882155</td>\n",
       "      <td>1685905200</td>\n",
       "      <td>867</td>\n",
       "      <td>Lecce</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>Bologna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1030674</td>\n",
       "      <td>1686509100</td>\n",
       "      <td>515</td>\n",
       "      <td>Spezia</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>504</td>\n",
       "      <td>Verona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fixtueID   timestamp  home_team_id home_team_name  home_goals  \\\n",
       "382    882140  1685278800           500        Bologna           2   \n",
       "383    882149  1685278800          1579          Monza           0   \n",
       "384    882147  1685289600           487          Lazio           3   \n",
       "385    882142  1685299500           496       Juventus           0   \n",
       "386    882156  1685730600           488       Sassuolo           1   \n",
       "387    882158  1685809800           503         Torino           0   \n",
       "388    882152  1685818800           511         Empoli           0   \n",
       "389    882151  1685818800           520      Cremonese           2   \n",
       "390    882153  1685896200           492         Napoli           2   \n",
       "391    882157  1685905200           489       AC Milan           3   \n",
       "392    882154  1685905200           497        AS Roma           2   \n",
       "393    882159  1685905200           494        Udinese           0   \n",
       "394    882150  1685905200           499       Atalanta           5   \n",
       "395    882155  1685905200           867          Lecce           2   \n",
       "396   1030674  1686509100           515         Spezia           1   \n",
       "\n",
       "     away_goals  away_team_id away_team_name  \n",
       "382           2           492         Napoli  \n",
       "383           1           867          Lecce  \n",
       "384           2           520      Cremonese  \n",
       "385           1           489       AC Milan  \n",
       "386           3           502     Fiorentina  \n",
       "387           1           505          Inter  \n",
       "388           2           487          Lazio  \n",
       "389           0           514    Salernitana  \n",
       "390           0           498      Sampdoria  \n",
       "391           1           504         Verona  \n",
       "392           1           515         Spezia  \n",
       "393           1           496       Juventus  \n",
       "394           2          1579          Monza  \n",
       "395           3           500        Bologna  \n",
       "396           3           504         Verona  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_SERIE22.iloc[-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixtueID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>home_team_id</th>\n",
       "      <th>home_team_name</th>\n",
       "      <th>home_goals</th>\n",
       "      <th>away_goals</th>\n",
       "      <th>away_team_id</th>\n",
       "      <th>away_team_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>868064</td>\n",
       "      <td>1666204200</td>\n",
       "      <td>40</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48</td>\n",
       "      <td>West Ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>868063</td>\n",
       "      <td>1666204200</td>\n",
       "      <td>34</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45</td>\n",
       "      <td>Everton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>868056</td>\n",
       "      <td>1666204200</td>\n",
       "      <td>35</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "      <td>Southampton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>868065</td>\n",
       "      <td>1666206900</td>\n",
       "      <td>33</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>Tottenham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>868060</td>\n",
       "      <td>1666290600</td>\n",
       "      <td>36</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66</td>\n",
       "      <td>Aston Villa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>1035297</td>\n",
       "      <td>1701115200</td>\n",
       "      <td>36</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39</td>\n",
       "      <td>Wolves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>1141323</td>\n",
       "      <td>1701184500</td>\n",
       "      <td>2006</td>\n",
       "      <td>Baltika</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>597</td>\n",
       "      <td>Lokomotiv Moscow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>1141319</td>\n",
       "      <td>1701262800</td>\n",
       "      <td>1080</td>\n",
       "      <td>Orenburg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>558</td>\n",
       "      <td>Spartak Moscow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>1141321</td>\n",
       "      <td>1701270900</td>\n",
       "      <td>1088</td>\n",
       "      <td>Dinamo Moscow</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>596</td>\n",
       "      <td>Zenit Saint Petersburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>1141325</td>\n",
       "      <td>1701270900</td>\n",
       "      <td>779</td>\n",
       "      <td>FC Rostov</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>555</td>\n",
       "      <td>CSKA Moscow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1620 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixtueID   timestamp  home_team_id     home_team_name  home_goals  \\\n",
       "0       868064  1666204200            40          Liverpool         1.0   \n",
       "1       868063  1666204200            34          Newcastle         1.0   \n",
       "2       868056  1666204200            35        Bournemouth         0.0   \n",
       "3       868065  1666206900            33  Manchester United         2.0   \n",
       "4       868060  1666290600            36             Fulham         3.0   \n",
       "...        ...         ...           ...                ...         ...   \n",
       "1615   1035297  1701115200            36             Fulham         3.0   \n",
       "1616   1141323  1701184500          2006            Baltika         2.0   \n",
       "1617   1141319  1701262800          1080           Orenburg         1.0   \n",
       "1618   1141321  1701270900          1088      Dinamo Moscow         1.0   \n",
       "1619   1141325  1701270900           779          FC Rostov         1.0   \n",
       "\n",
       "      away_goals  away_team_id          away_team_name  \n",
       "0            0.0            48                West Ham  \n",
       "1            0.0            45                 Everton  \n",
       "2            1.0            41             Southampton  \n",
       "3            0.0            47               Tottenham  \n",
       "4            0.0            66             Aston Villa  \n",
       "...          ...           ...                     ...  \n",
       "1615         2.0            39                  Wolves  \n",
       "1616         2.0           597        Lokomotiv Moscow  \n",
       "1617         0.0           558          Spartak Moscow  \n",
       "1618         0.0           596  Zenit Saint Petersburg  \n",
       "1619         1.0           555             CSKA Moscow  \n",
       "\n",
       "[1620 rows x 8 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   \n",
    "general_eng = data_PL22.iloc[100:410].reset_index(drop=True)\n",
    "general_rus = (data_RFPL22.iloc[80:]).reset_index(drop=True)\n",
    "general_bundes = data_BUNDES22.iloc[100:318].reset_index(drop=True)\n",
    "general_italy = data_SERIE22.iloc[100:].reset_index(drop=True)\n",
    "all_general = general_eng.merge(general_rus, how='outer')\n",
    "all_general = all_general.merge(general_bundes, how='outer')\n",
    "all_general = all_general.merge(general_italy, how='outer')\n",
    "all_general = all_general.merge(all_data, how='outer')\n",
    "all_general.sort_values('timestamp').reset_index(drop=True, inplace=True)\n",
    "all_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data = pd.read_csv('match_data.csv',index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data['points'] = match_data.apply(lambda row: 3 if row['goals'] > match_data[(match_data['fixtureID'] == row['fixtureID']) & (match_data['team_id'] != row['team_id'])]['goals'].max() else (0 if row['goals'] < match_data[(match_data['fixtureID'] == row['fixtureID']) & (match_data['team_id'] != row['team_id'])]['goals'].max() else 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x  y\n",
    "def create_samples(data):\n",
    "    y = []\n",
    "    x = []\n",
    "    for _, row in data.iterrows():\n",
    "        new_line_home = []\n",
    "        new_line_away = []\n",
    "        home_team_data = match_data[(match_data['team_id'] == row['home_team_id']) & (match_data['time'] < row['timestamp'])][-10::]\n",
    "        away_team_data = match_data[(match_data['team_id'] == row['away_team_id']) & (match_data['time'] < row['timestamp'])][-10::]\n",
    "        points_last_home = match_data[(match_data['team_id'] == row['home_team_id']) & (match_data['time'] < row['timestamp'])][-5::]['points'].sum()\n",
    "        points_last_away =  match_data[(match_data['team_id'] == row['home_team_id']) & (match_data['time'] < row['timestamp'])][-5::]['points'].sum()\n",
    "        hh = home_team_data[home_team_data['position'] == 'home']\n",
    "        ha = home_team_data[home_team_data['position'] == 'away']\n",
    "\n",
    "        ah = away_team_data[away_team_data['position'] == 'home']\n",
    "        aa = away_team_data[away_team_data['position'] == 'away']\n",
    "\n",
    "        new_line_home.extend([row['home_team_id'], row['timestamp'], hh['goals'].mean(), ha['goals'].mean(), hh['shots'].mean(), ha['shots'].mean(), hh['xg'].mean(), ha['xg'].mean(), points_last_home])\n",
    "        new_line_away.extend([row['away_team_id'], row['timestamp'], ah['goals'].mean(), aa['goals'].mean(), ah['shots'].mean(), aa['shots'].mean(), ah['xg'].mean(), aa['xg'].mean(), points_last_away])\n",
    "\n",
    "        x.extend([new_line_home, new_line_away]) \n",
    "        y.extend([[0.5, min(row['home_goals'], 6) if row['home_goals'] > 0 else 0], [1, min(row['away_goals'], 6) if row['away_goals'] > 0 else 0]])\n",
    "        #y.extend([[1, row['home_goals']], [2, row['away_goals']]])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team_id</th>\n",
       "      <th>time</th>\n",
       "      <th>mean_goals_home</th>\n",
       "      <th>mean_goals_away</th>\n",
       "      <th>shots_home</th>\n",
       "      <th>shots_away</th>\n",
       "      <th>xg_home</th>\n",
       "      <th>xg_away</th>\n",
       "      <th>points</th>\n",
       "      <th>position</th>\n",
       "      <th>actual_goal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>1666204200</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>1666204200</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>1666204200</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>1666206900</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.20</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>1666290600</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>39</td>\n",
       "      <td>1701115200</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.164000</td>\n",
       "      <td>1.306000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>597</td>\n",
       "      <td>1701184500</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>4.75</td>\n",
       "      <td>1.811667</td>\n",
       "      <td>1.830000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>558</td>\n",
       "      <td>1701262800</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>596</td>\n",
       "      <td>1701270900</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>555</td>\n",
       "      <td>1701270900</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1620 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      team_id        time  mean_goals_home  mean_goals_away  shots_home  \\\n",
       "0          48  1666204200         1.200000         0.600000    4.200000   \n",
       "1          45  1666204200         0.600000         1.000000    3.800000   \n",
       "2          41  1666204200         1.200000         0.600000    6.000000   \n",
       "3          47  1666206900         3.000000         1.400000    8.000000   \n",
       "4          66  1666290600         0.800000         0.600000    3.600000   \n",
       "...       ...         ...              ...              ...         ...   \n",
       "1615       39  1701115200         1.600000         1.400000    3.200000   \n",
       "1616      597  1701184500         1.666667         1.750000    4.166667   \n",
       "1617      558  1701262800         1.800000         0.600000    6.600000   \n",
       "1618      596  1701270900         1.857143         1.666667    4.285714   \n",
       "1619      555  1701270900         1.833333         1.500000    4.500000   \n",
       "\n",
       "      shots_away   xg_home   xg_away  points  position  actual_goal  \n",
       "0           3.60  1.200000  0.600000       8       1.0          0.0  \n",
       "1           3.60  0.600000  1.000000       9       1.0          0.0  \n",
       "2           1.40  1.200000  0.600000       9       1.0          1.0  \n",
       "3           4.20  3.000000  1.400000      10       1.0          0.0  \n",
       "4           3.80  0.800000  0.600000       4       1.0          0.0  \n",
       "...          ...       ...       ...     ...       ...          ...  \n",
       "1615        3.80  1.164000  1.306000       4       1.0          2.0  \n",
       "1616        4.75  1.811667  1.830000       4       1.0          2.0  \n",
       "1617        2.80  1.800000  0.434000       3       1.0          0.0  \n",
       "1618        7.00  1.857143  1.666667      13       1.0          0.0  \n",
       "1619        3.25  1.833333  1.500000      10       1.0          1.0  \n",
       "\n",
       "[1620 rows x 11 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = create_samples(all_general)\n",
    "x = pd.DataFrame(x, columns=['team_id','time', 'mean_goals_home', 'mean_goals_away', 'shots_home', 'shots_away', 'xg_home', 'xg_away', 'points'])\n",
    "y = pd.DataFrame(y, columns=['position', 'actual_goal'])\n",
    "all_data = pd.concat([x, y], axis=1)\n",
    "all_data_home = all_data[all_data['position'] == 0.5].reset_index(drop=True)\n",
    "all_data_away = all_data[all_data['position'] == 1].reset_index(drop=True)\n",
    "all_data_away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_england_22 = pd.read_csv('./elo/csv/England/2022/ratingElo2022_2023PL.csv')\n",
    "elo_england_23 = pd.read_csv('./elo/csv/England/2023/ratingElo2023_2024PL.csv')\n",
    "\n",
    "elo_england_22.set_index('id', inplace=True)\n",
    "elo_england_23.set_index('id', inplace=True)\n",
    "\n",
    "elo_england_23['prev_elos'] = elo_england_23.apply(lambda row: elo_england_22.loc[row.name]['prev_elos'] + row['prev_elos'] if row.name in elo_england_22.index else row['prev_elos'], axis=1)\n",
    "elo_england_23.reset_index(inplace=True)\n",
    "\n",
    "elo_russian_22 = pd.read_csv('./elo/csv/Russia/2022/ratingElo2022_2023RFPL.csv')\n",
    "elo_russian_23 = pd.read_csv('./elo/csv/Russia/2023/ratingElo2023_2024RFPL.csv')\n",
    "\n",
    "elo_russian_22.set_index('id', inplace=True)\n",
    "elo_russian_23.set_index('id', inplace=True)\n",
    "\n",
    "elo_russian_23['prev_elos'] = elo_russian_23.apply(lambda row: elo_russian_22.loc[row.name]['prev_elos'] + row['prev_elos'] if row.name in elo_russian_22.index else row['prev_elos'], axis=1)\n",
    "elo_russian_23.reset_index(inplace=True)\n",
    "\n",
    "elo_bundes_22 = pd.read_csv('./elo/csv/Germany/2022/ratingElo2022_2023Bundes.csv')\n",
    "elo_bundes_23 = pd.read_csv('./elo/csv/Germany/2023/ratingElo2023_2024Bundes.csv')\n",
    "\n",
    "elo_bundes_22.set_index('id', inplace=True)\n",
    "elo_bundes_23.set_index('id', inplace=True)\n",
    "\n",
    "elo_bundes_23['prev_elos'] = elo_bundes_23.apply(lambda row: elo_bundes_22.loc[row.name]['prev_elos'] + row['prev_elos'] if row.name in elo_bundes_22.index else row['prev_elos'], axis=1)\n",
    "elo_bundes_23.reset_index(inplace=True)\n",
    "\n",
    "elo_italy_22 = pd.read_csv('./elo/csv/Italy/2022/ratingElo2022_2023Italy.csv')\n",
    "elo_italy_23 = pd.read_csv('./elo/csv/Italy/2023/ratingElo2023_2024Italy.csv')\n",
    "\n",
    "elo_italy_22.set_index('id', inplace=True)\n",
    "elo_italy_23.set_index('id', inplace=True)\n",
    "\n",
    "elo_italy_23['prev_elos'] = elo_italy_23.apply(lambda row: elo_italy_22.loc[row.name]['prev_elos'] + row['prev_elos'] if row.name in elo_italy_22.index else row['prev_elos'], axis=1)\n",
    "elo_italy_23.reset_index(inplace=True)\n",
    "\n",
    "all_elo = pd.concat([elo_england_23, elo_russian_23, elo_bundes_23, elo_italy_23])\n",
    "all_elo.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team_id</th>\n",
       "      <th>time</th>\n",
       "      <th>mean_goals_home</th>\n",
       "      <th>mean_goals_away</th>\n",
       "      <th>shots_home</th>\n",
       "      <th>shots_away</th>\n",
       "      <th>xg_home</th>\n",
       "      <th>xg_away</th>\n",
       "      <th>points</th>\n",
       "      <th>position</th>\n",
       "      <th>actual_goal</th>\n",
       "      <th>elo</th>\n",
       "      <th>diff_elo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>team_id</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015265</td>\n",
       "      <td>-0.200580</td>\n",
       "      <td>-0.176381</td>\n",
       "      <td>-0.222240</td>\n",
       "      <td>-0.181999</td>\n",
       "      <td>-0.204746</td>\n",
       "      <td>-0.194536</td>\n",
       "      <td>-0.174004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.122803</td>\n",
       "      <td>-0.612309</td>\n",
       "      <td>-0.163881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>-0.015265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.032796</td>\n",
       "      <td>0.054360</td>\n",
       "      <td>-0.069162</td>\n",
       "      <td>-0.021635</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>0.058805</td>\n",
       "      <td>-0.014563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>0.043195</td>\n",
       "      <td>-0.007947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_goals_home</th>\n",
       "      <td>-0.200580</td>\n",
       "      <td>-0.032796</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.356006</td>\n",
       "      <td>0.703226</td>\n",
       "      <td>0.352609</td>\n",
       "      <td>0.868636</td>\n",
       "      <td>0.331997</td>\n",
       "      <td>0.527629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.207006</td>\n",
       "      <td>0.494301</td>\n",
       "      <td>0.439880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_goals_away</th>\n",
       "      <td>-0.176381</td>\n",
       "      <td>0.054360</td>\n",
       "      <td>0.356006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.346475</td>\n",
       "      <td>0.638349</td>\n",
       "      <td>0.360905</td>\n",
       "      <td>0.846960</td>\n",
       "      <td>0.536953</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.210826</td>\n",
       "      <td>0.450779</td>\n",
       "      <td>0.408540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shots_home</th>\n",
       "      <td>-0.222240</td>\n",
       "      <td>-0.069162</td>\n",
       "      <td>0.703226</td>\n",
       "      <td>0.346475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.387260</td>\n",
       "      <td>0.741608</td>\n",
       "      <td>0.323310</td>\n",
       "      <td>0.468608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.215496</td>\n",
       "      <td>0.461897</td>\n",
       "      <td>0.378985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shots_away</th>\n",
       "      <td>-0.181999</td>\n",
       "      <td>-0.021635</td>\n",
       "      <td>0.352609</td>\n",
       "      <td>0.638349</td>\n",
       "      <td>0.387260</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.381537</td>\n",
       "      <td>0.682726</td>\n",
       "      <td>0.447171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.225154</td>\n",
       "      <td>0.449046</td>\n",
       "      <td>0.401524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xg_home</th>\n",
       "      <td>-0.204746</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>0.868636</td>\n",
       "      <td>0.360905</td>\n",
       "      <td>0.741608</td>\n",
       "      <td>0.381537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.361010</td>\n",
       "      <td>0.522026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.210257</td>\n",
       "      <td>0.494901</td>\n",
       "      <td>0.436549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xg_away</th>\n",
       "      <td>-0.194536</td>\n",
       "      <td>0.058805</td>\n",
       "      <td>0.331997</td>\n",
       "      <td>0.846960</td>\n",
       "      <td>0.323310</td>\n",
       "      <td>0.682726</td>\n",
       "      <td>0.361010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.473036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.214924</td>\n",
       "      <td>0.445106</td>\n",
       "      <td>0.403981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>points</th>\n",
       "      <td>-0.174004</td>\n",
       "      <td>-0.014563</td>\n",
       "      <td>0.527629</td>\n",
       "      <td>0.536953</td>\n",
       "      <td>0.468608</td>\n",
       "      <td>0.447171</td>\n",
       "      <td>0.522026</td>\n",
       "      <td>0.473036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.189763</td>\n",
       "      <td>0.592032</td>\n",
       "      <td>0.507818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>position</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_goal</th>\n",
       "      <td>-0.122803</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>0.207006</td>\n",
       "      <td>0.210826</td>\n",
       "      <td>0.215496</td>\n",
       "      <td>0.225154</td>\n",
       "      <td>0.210257</td>\n",
       "      <td>0.214924</td>\n",
       "      <td>0.189763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.219863</td>\n",
       "      <td>0.321128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elo</th>\n",
       "      <td>-0.612309</td>\n",
       "      <td>0.043195</td>\n",
       "      <td>0.494301</td>\n",
       "      <td>0.450779</td>\n",
       "      <td>0.461897</td>\n",
       "      <td>0.449046</td>\n",
       "      <td>0.494901</td>\n",
       "      <td>0.445106</td>\n",
       "      <td>0.592032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.219863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.577191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diff_elo</th>\n",
       "      <td>-0.163881</td>\n",
       "      <td>-0.007947</td>\n",
       "      <td>0.439880</td>\n",
       "      <td>0.408540</td>\n",
       "      <td>0.378985</td>\n",
       "      <td>0.401524</td>\n",
       "      <td>0.436549</td>\n",
       "      <td>0.403981</td>\n",
       "      <td>0.507818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.321128</td>\n",
       "      <td>0.577191</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  team_id      time  mean_goals_home  mean_goals_away  \\\n",
       "team_id          1.000000 -0.015265        -0.200580        -0.176381   \n",
       "time            -0.015265  1.000000        -0.032796         0.054360   \n",
       "mean_goals_home -0.200580 -0.032796         1.000000         0.356006   \n",
       "mean_goals_away -0.176381  0.054360         0.356006         1.000000   \n",
       "shots_home      -0.222240 -0.069162         0.703226         0.346475   \n",
       "shots_away      -0.181999 -0.021635         0.352609         0.638349   \n",
       "xg_home         -0.204746 -0.066583         0.868636         0.360905   \n",
       "xg_away         -0.194536  0.058805         0.331997         0.846960   \n",
       "points          -0.174004 -0.014563         0.527629         0.536953   \n",
       "position              NaN       NaN              NaN              NaN   \n",
       "actual_goal     -0.122803 -0.000240         0.207006         0.210826   \n",
       "elo             -0.612309  0.043195         0.494301         0.450779   \n",
       "diff_elo        -0.163881 -0.007947         0.439880         0.408540   \n",
       "\n",
       "                 shots_home  shots_away   xg_home   xg_away    points  \\\n",
       "team_id           -0.222240   -0.181999 -0.204746 -0.194536 -0.174004   \n",
       "time              -0.069162   -0.021635 -0.066583  0.058805 -0.014563   \n",
       "mean_goals_home    0.703226    0.352609  0.868636  0.331997  0.527629   \n",
       "mean_goals_away    0.346475    0.638349  0.360905  0.846960  0.536953   \n",
       "shots_home         1.000000    0.387260  0.741608  0.323310  0.468608   \n",
       "shots_away         0.387260    1.000000  0.381537  0.682726  0.447171   \n",
       "xg_home            0.741608    0.381537  1.000000  0.361010  0.522026   \n",
       "xg_away            0.323310    0.682726  0.361010  1.000000  0.473036   \n",
       "points             0.468608    0.447171  0.522026  0.473036  1.000000   \n",
       "position                NaN         NaN       NaN       NaN       NaN   \n",
       "actual_goal        0.215496    0.225154  0.210257  0.214924  0.189763   \n",
       "elo                0.461897    0.449046  0.494901  0.445106  0.592032   \n",
       "diff_elo           0.378985    0.401524  0.436549  0.403981  0.507818   \n",
       "\n",
       "                 position  actual_goal       elo  diff_elo  \n",
       "team_id               NaN    -0.122803 -0.612309 -0.163881  \n",
       "time                  NaN    -0.000240  0.043195 -0.007947  \n",
       "mean_goals_home       NaN     0.207006  0.494301  0.439880  \n",
       "mean_goals_away       NaN     0.210826  0.450779  0.408540  \n",
       "shots_home            NaN     0.215496  0.461897  0.378985  \n",
       "shots_away            NaN     0.225154  0.449046  0.401524  \n",
       "xg_home               NaN     0.210257  0.494901  0.436549  \n",
       "xg_away               NaN     0.214924  0.445106  0.403981  \n",
       "points                NaN     0.189763  0.592032  0.507818  \n",
       "position              NaN          NaN       NaN       NaN  \n",
       "actual_goal           NaN     1.000000  0.219863  0.321128  \n",
       "elo                   NaN     0.219863  1.000000  0.577191  \n",
       "diff_elo              NaN     0.321128  0.577191  1.000000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = [all_data_home, all_data_away]\n",
    "for item in samples:\n",
    "    add_column_elo(item)\n",
    "all_data_home['diff_elo'] = all_data_home['elo'] - all_data_away['elo']\n",
    "all_data_away['diff_elo'] = all_data_away['elo'] - all_data_home['elo']\n",
    "all_data_home.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = 1134 rows, 9 cols\n",
      "x_valid.shape = 340 rows, 9 cols\n",
      "x_test.shape = 146 rows, 9 cols\n"
     ]
    }
   ],
   "source": [
    "#    train / valid\n",
    "x_home = all_data_home.drop(['time','position' ,'actual_goal', 'elo'], axis=1)\n",
    "x_away = all_data_away.drop(['time','position' ,'actual_goal', 'elo'], axis=1)\n",
    "scaler_home = MinMaxScaler()\n",
    "scaler_away = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "x_home = scaler_home.fit_transform(x_home)\n",
    "x_away = scaler_away.fit_transform(x_away)\n",
    "joblib.dump(scaler_home, 'scaler_home_new.save')\n",
    "joblib.dump(scaler_away, 'scaler_away_new.save')\n",
    "\n",
    "x_train_home, x_valid_home = train_test_split(\n",
    "    x_home, train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "y_train_home, y_valid_home = train_test_split(\n",
    "    all_data_home[\"actual_goal\"], train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "\n",
    "x_train_away, x_valid_away = train_test_split(\n",
    "    x_away, train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "y_train_away, y_valid_away = train_test_split(\n",
    "    all_data_away[\"actual_goal\"], train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "\n",
    "#    valid / test\n",
    "\n",
    "x_valid_home, x_test_home = train_test_split(\n",
    "    x_valid_home, train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "y_valid_home, y_test_home = train_test_split(\n",
    "    y_valid_home, train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "\n",
    "x_valid_away, x_test_away = train_test_split(\n",
    "    x_valid_away, train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "y_valid_away, y_test_away = train_test_split(\n",
    "    y_valid_away, train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "\n",
    "print(\"x_train.shape = {} rows, {} cols\".format(*x_train_home.shape))\n",
    "print(\"x_valid.shape = {} rows, {} cols\".format(*x_valid_home.shape))\n",
    "print(\"x_test.shape = {} rows, {} cols\".format(*x_test_home.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graphics_goals(y_test, y_pred, msg):\n",
    "    plt.plot(y_test, label='Actual')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(msg)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "def plot_validation_curves(train_scores: np.array,\n",
    "                           valid_scores: np.array,\n",
    "                           figsize: Tuple[int, int] = (8, 8)\n",
    "                          ):\n",
    "    \"\"\"\n",
    "       \n",
    "      .  \n",
    "           , \n",
    "         .\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_scores: np.array\n",
    "             .\n",
    "\n",
    "    valid_scores: np.array\n",
    "             .\n",
    "\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    #plt.subplot(121)\n",
    "    plt.title(\"Validation Curves\", size=15)\n",
    "    plt.plot(\n",
    "        range(train_scores.shape[0]),\n",
    "        np.mean(train_scores, axis=1),\n",
    "        label=\"train\",\n",
    "        linewidth=3,\n",
    "        marker=\"s\"\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        x=range(train_scores.shape[0]),\n",
    "        y1=np.mean(train_scores, axis=1)-np.std(train_scores, axis=1),\n",
    "        y2=np.mean(train_scores, axis=1)+np.std(train_scores, axis=1),\n",
    "        alpha=0.25\n",
    "    )\n",
    "    #plt.subplot(121)\n",
    "    plt.plot(\n",
    "        range(train_scores.shape[0]),\n",
    "        np.mean(valid_scores, axis=1),\n",
    "        label=\"valid\",\n",
    "        linewidth=3,\n",
    "        marker=\"s\"\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        x=range(train_scores.shape[0]),\n",
    "        y1=np.mean(valid_scores, axis=1)-np.std(valid_scores, axis=1),\n",
    "        y2=np.mean(valid_scores, axis=1)+np.std(valid_scores, axis=1),\n",
    "        alpha=0.25\n",
    "    )\n",
    "    plt.legend(loc=\"best\", fontsize=14)\n",
    "    plt.ylabel(\"mae\", size=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_models(models, x_train_home, y_train_home, x_train_away, \n",
    "                       y_train_away, x_valid_home, y_valid_home, x_valid_away, y_valid_away, x_test_home, y_test_home, x_test_away, y_test_away):\n",
    "    results_home = {}\n",
    "    results_away = {}\n",
    "    results_overall = {}\n",
    "\n",
    "    results_home_test = {}\n",
    "    results_away_test = {}\n",
    "    results_overall_test = {}\n",
    "    for model in models:\n",
    "        model_name = model.__class__.__name__\n",
    "        model_home = model.fit(x_train_home, y_train_home)\n",
    "        y_pred_home = model_home.predict(x_valid_home)\n",
    "        y_pred_home_test = model_home.predict(x_test_home)\n",
    "        mse_home = mean_squared_error(y_valid_home, y_pred_home)\n",
    "        mae_home = mean_absolute_error(y_valid_home, y_pred_home)\n",
    "        mse_home_test = mean_squared_error(y_test_home, y_pred_home_test)\n",
    "        mae_home_test = mean_absolute_error(y_test_home, y_pred_home_test)\n",
    "\n",
    "        model_away = model.fit(x_train_away, y_train_away)\n",
    "        y_pred_away = model_away.predict(x_valid_away)\n",
    "        y_pred_away_test = model_away.predict(x_test_away)\n",
    "        mse_away = mean_squared_error(y_valid_away, y_pred_away)\n",
    "        mae_away = mean_absolute_error(y_valid_away, y_pred_away)\n",
    "        mse_away_test = mean_squared_error(y_test_away, y_pred_away_test)\n",
    "        mae_away_test = mean_absolute_error(y_test_away, y_pred_away_test)\n",
    "        if (model_name == 'Pipeline'):\n",
    "            joblib.dump(model_home, 'model_poly_home_new.joblib')\n",
    "            joblib.dump(model_away, 'model_poly_away_new.joblib')\n",
    "        #   \n",
    "        if (model_name == 'TestRandomForestRegressor'):\n",
    "            train_scores, valid_scores = validation_curve(\n",
    "                    X=x_train_home,\n",
    "                    y=y_train_home,\n",
    "                    estimator=model,\n",
    "                    param_range=range(1, 15),\n",
    "                    param_name=\"min_samples_leaf\",\n",
    "                    scoring=\"neg_mean_absolute_error\",\n",
    "                    n_jobs=1,\n",
    "                    cv=3,\n",
    "            )\n",
    "            plot_validation_curves(\n",
    "                train_scores, valid_scores\n",
    "            )\n",
    "            train_scores, valid_scores = validation_curve(\n",
    "                    X=x_train_away,\n",
    "                    y=y_train_away,\n",
    "                    estimator=model,\n",
    "                    param_range=range(1, 15),\n",
    "                    param_name=\"min_samples_leaf\",\n",
    "                    scoring=\"neg_mean_absolute_error\",\n",
    "                    n_jobs=1,\n",
    "                    cv=3,\n",
    "            )\n",
    "            plot_validation_curves(\n",
    "                train_scores, valid_scores\n",
    "            )        \n",
    "        results_home[model_name] = {'MSE': mse_home, 'MAE': mae_home, 'MIN': min(y_pred_home), 'MAX': max(y_pred_home)}\n",
    "        results_away[model_name] = {'MSE': mse_away, 'MAE': mae_away, 'MIN': min(y_pred_away), 'MAX': max(y_pred_away)}\n",
    "        results_overall[model_name] = {'MSE': (mse_home + mse_away) / 2, 'MAE': (mae_home + mae_away) / 2, 'MIN': min(min(y_pred_home), min(y_pred_away)), 'MAX': max(max(y_pred_home),  max(y_pred_away))}\n",
    "        #get_graphics_goals(y_valid_home, y_pred_home, model_name)\n",
    "        #get_graphics_goals(y_valid_away, y_pred_away, model_name)\n",
    "\n",
    "        results_home_test[model_name] = {'MSE': mse_home_test, 'MAE': mae_home_test, 'MIN': min(y_pred_home_test), 'MAX': max(y_pred_home_test)}\n",
    "        results_away_test[model_name] = {'MSE': mse_away_test, 'MAE': mae_away_test, 'MIN': min(y_pred_away_test), 'MAX': max(y_pred_away_test)}\n",
    "        results_overall_test[model_name] = {'MSE': (mse_home_test + mse_away_test) / 2, 'MAE': (mae_home_test + mae_away_test) / 2, 'MIN': min(min(y_pred_home_test), min(y_pred_away_test)), 'MAX': max(max(y_pred_home_test),  max(y_pred_away_test))}\n",
    "\n",
    "    results_home = pd.DataFrame(results_home)\n",
    "    results_away = pd.DataFrame(results_away)\n",
    "    results_overall = pd.DataFrame(results_overall)\n",
    "\n",
    "    results_home_test = pd.DataFrame(results_home_test)\n",
    "    results_away_test = pd.DataFrame(results_away_test)\n",
    "    results_overall_test = pd.DataFrame(results_overall_test)\n",
    " \n",
    "    print(' :\\n')\n",
    "    print('   :')\n",
    "    print(results_home)\n",
    "    print('\\n   :')\n",
    "    print(results_away)\n",
    "    print('\\n :')\n",
    "    print(results_overall)\n",
    "    print(' :\\n')\n",
    "    print('   :')\n",
    "    print(results_home_test)\n",
    "    print('\\n   :')\n",
    "    print(results_away_test)\n",
    "    print('\\n :')\n",
    "    print(results_overall_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_neuron_results(model, x, y, label, label_gra):\n",
    "    test_loss, test_mae = model.evaluate(x, y)\n",
    "    y_pred = model.predict(x)\n",
    "    print(label)\n",
    "    print(f\"Test Loss (MSE): {test_loss}\")\n",
    "    #print(f\"Test RMSE: {np.sqrt(test_loss_home)}\")\n",
    "    print(f\"Test MAE: {test_mae}\")\n",
    "    print(f\"MIN: {min(y_pred)}\")\n",
    "    print(f\"MAX: {max(y_pred)}\")\n",
    "    #get_graphics_goals(y, y_pred, label_gra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_network(x_train_home, y_train_home, x_train_away, \n",
    "                       y_train_away, x_valid_home, y_valid_home, x_valid_away, y_valid_away, x_test_home, y_test_home, x_test_away, y_test_away):\n",
    "    # ------------  ( ) ----------------------------\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "    \n",
    "    input_shape = (9, )\n",
    "\n",
    "    model_away = keras.Sequential([\n",
    "        keras.layers.Dense(256, input_shape=input_shape, activation='linear', kernel_regularizer=l2(0.01)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(128, activation='linear', kernel_regularizer=l2(0.01)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(64, activation='linear', kernel_regularizer=l2(0.01)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(16, activation='linear', kernel_regularizer=l2(0.01)),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model_home = keras.Sequential([\n",
    "        keras.layers.Dense(256, input_shape=input_shape, activation='linear', kernel_regularizer=l2(0.01)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(128, activation='linear', kernel_regularizer=l2(0.01)),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        keras.layers.Dense(64, activation='linear', kernel_regularizer=l2(0.01)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(16, activation='linear', kernel_regularizer=l2(0.01)),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "\n",
    "    model_away.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    model_home.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    early_stopping_home = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
    "    early_stopping_away = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
    "\n",
    "    train_away = model_away.fit(x_train_away, y_train_away, epochs=3000, shuffle=True, batch_size=32, validation_data=(x_valid_away, y_valid_away), callbacks=[early_stopping_away])\n",
    "    train_home = model_home.fit(x_train_home, y_train_home, epochs=3000, shuffle=True, batch_size=32, validation_data=(x_valid_home, y_valid_home), callbacks=[early_stopping_home])\n",
    "    \n",
    "    history = train_away.history\n",
    "    plt.plot(history['loss'], label='Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ------------  ( ) -----------------------------\n",
    "    print(' :\\n')  \n",
    "    print_neuron_results(model_home, x_valid_home, y_valid_home, ' :', ' ( )')\n",
    "    print_neuron_results(model_away, x_valid_away, y_valid_away, ' :', ' ( )')\n",
    "\n",
    "    print('\\n :\\n')  \n",
    "    print_neuron_results(model_home, x_test_home, y_test_home, ' :', ' ( )')\n",
    "    print_neuron_results(model_away, x_test_away, y_test_away, ' :', ' ( )')\n",
    "\n",
    "    model_home.save('home_model_new.h5');\n",
    "    model_away.save('away_model_new.h5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train_home, y_train_home, x_train_away, \n",
    "                       y_train_away, x_valid_home, y_valid_home, x_valid_away, y_valid_away, x_test_home, y_test_home, x_test_away, y_test_away):\n",
    "    # ------------  ( ,  , ) -----------------------------\n",
    "    models = [Ridge(random_state=25,alpha=1.0, max_iter=100000), \n",
    "\t          RandomForestRegressor(random_state=25, max_depth=2, min_samples_leaf=12, n_estimators=300, min_samples_split=3),\n",
    "              make_pipeline(PolynomialFeatures(degree=2, include_bias=False), Ridge(random_state=25, alpha=1.0))]\n",
    "              #xgb.XGBRegressor(random_state=25),\n",
    "              #cb.CatBoostRegressor(random_state=25)]\n",
    "    get_results_models(models, x_train_home, y_train_home, x_train_away, \n",
    "                       y_train_away, x_valid_home, y_valid_home, x_valid_away, y_valid_away, x_test_home, y_test_home, x_test_away, y_test_away)\n",
    "    \n",
    "    neuron_network(x_train_home, y_train_home, x_train_away, \n",
    "                       y_train_away, x_valid_home, y_valid_home, x_valid_away, y_valid_away, x_test_home, y_test_home, x_test_away, y_test_away)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :\n",
      "\n",
      "   :\n",
      "        Ridge  RandomForestRegressor  Pipeline\n",
      "MSE  1.595092               1.587604  1.610920\n",
      "MAE  0.992705               0.989907  0.996412\n",
      "MIN  0.384626               1.034473  0.384901\n",
      "MAX  2.885622               2.529410  3.138768\n",
      "\n",
      "   :\n",
      "        Ridge  RandomForestRegressor  Pipeline\n",
      "MSE  1.370610               1.360218  1.388818\n",
      "MAE  0.954379               0.929683  0.958394\n",
      "MIN  0.422113               0.937933  0.472732\n",
      "MAX  2.234556               1.999612  2.567490\n",
      "\n",
      " :\n",
      "        Ridge  RandomForestRegressor  Pipeline\n",
      "MSE  1.482851               1.473911  1.499869\n",
      "MAE  0.973542               0.959795  0.977403\n",
      "MIN  0.384626               0.937933  0.384901\n",
      "MAX  2.885622               2.529410  3.138768\n",
      " :\n",
      "\n",
      "   :\n",
      "        Ridge  RandomForestRegressor  Pipeline\n",
      "MSE  1.407750               1.345718  1.387380\n",
      "MAE  0.927455               0.901529  0.919528\n",
      "MIN  0.526807               1.038329  0.626554\n",
      "MAX  3.050405               2.565485  3.110639\n",
      "\n",
      "   :\n",
      "        Ridge  RandomForestRegressor  Pipeline\n",
      "MSE  0.912677               0.934655  0.926954\n",
      "MAE  0.759944               0.741842  0.755276\n",
      "MIN  0.477964               0.940432  0.641629\n",
      "MAX  2.075227               1.903798  2.295321\n",
      "\n",
      " :\n",
      "        Ridge  RandomForestRegressor  Pipeline\n",
      "MSE  1.160214               1.140186  1.157167\n",
      "MAE  0.843700               0.821686  0.837402\n",
      "MIN  0.477964               0.940432  0.626554\n",
      "MAX  3.050405               2.565485  3.110639\n",
      "Epoch 1/3000\n",
      "36/36 [==============================] - 5s 17ms/step - loss: 4.4924 - mae: 0.9829 - val_loss: 4.1435 - val_mae: 0.9302\n",
      "Epoch 2/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 3.9709 - mae: 0.9059 - val_loss: 3.8326 - val_mae: 0.9273\n",
      "Epoch 3/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 3.6432 - mae: 0.8925 - val_loss: 3.5272 - val_mae: 0.9648\n",
      "Epoch 4/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 3.3430 - mae: 0.8936 - val_loss: 3.2892 - val_mae: 0.9338\n",
      "Epoch 5/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 3.0800 - mae: 0.8791 - val_loss: 3.0666 - val_mae: 0.9307\n",
      "Epoch 6/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.8985 - mae: 0.8807 - val_loss: 2.8736 - val_mae: 0.9325\n",
      "Epoch 7/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.6791 - mae: 0.8685 - val_loss: 2.7038 - val_mae: 0.9406\n",
      "Epoch 8/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.5393 - mae: 0.8726 - val_loss: 2.5620 - val_mae: 0.9367\n",
      "Epoch 9/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.3928 - mae: 0.8762 - val_loss: 2.4368 - val_mae: 0.9587\n",
      "Epoch 10/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 2.2585 - mae: 0.8594 - val_loss: 2.3285 - val_mae: 0.9617\n",
      "Epoch 11/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.1486 - mae: 0.8696 - val_loss: 2.2220 - val_mae: 0.9420\n",
      "Epoch 12/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.0209 - mae: 0.8489 - val_loss: 2.1657 - val_mae: 0.9865\n",
      "Epoch 13/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.0154 - mae: 0.8765 - val_loss: 2.0654 - val_mae: 0.9406\n",
      "Epoch 14/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.8887 - mae: 0.8587 - val_loss: 2.0022 - val_mae: 0.9609\n",
      "Epoch 15/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.8153 - mae: 0.8607 - val_loss: 1.9336 - val_mae: 0.9488\n",
      "Epoch 16/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.7872 - mae: 0.8627 - val_loss: 1.8998 - val_mae: 0.9253\n",
      "Epoch 17/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.7493 - mae: 0.8610 - val_loss: 1.8740 - val_mae: 0.9225\n",
      "Epoch 18/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.7015 - mae: 0.8674 - val_loss: 1.7905 - val_mae: 0.9522\n",
      "Epoch 19/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6556 - mae: 0.8670 - val_loss: 1.7585 - val_mae: 0.9334\n",
      "Epoch 20/3000\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.5965 - mae: 0.8572 - val_loss: 1.7198 - val_mae: 0.9505\n",
      "Epoch 21/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5515 - mae: 0.8513 - val_loss: 1.6932 - val_mae: 0.9514\n",
      "Epoch 22/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5472 - mae: 0.8617 - val_loss: 1.6664 - val_mae: 0.9557\n",
      "Epoch 23/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5416 - mae: 0.8653 - val_loss: 1.6517 - val_mae: 0.9688\n",
      "Epoch 24/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.4974 - mae: 0.8634 - val_loss: 1.6284 - val_mae: 0.9681\n",
      "Epoch 25/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.4643 - mae: 0.8563 - val_loss: 1.6000 - val_mae: 0.9474\n",
      "Epoch 26/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.4518 - mae: 0.8515 - val_loss: 1.5888 - val_mae: 0.9609\n",
      "Epoch 27/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.4399 - mae: 0.8681 - val_loss: 1.5784 - val_mae: 0.9372\n",
      "Epoch 28/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.4163 - mae: 0.8602 - val_loss: 1.5580 - val_mae: 0.9590\n",
      "Epoch 29/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.4132 - mae: 0.8558 - val_loss: 1.5571 - val_mae: 0.9727\n",
      "Epoch 30/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.3996 - mae: 0.8653 - val_loss: 1.5365 - val_mae: 0.9406\n",
      "Epoch 31/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.3824 - mae: 0.8618 - val_loss: 1.5237 - val_mae: 0.9512\n",
      "Epoch 32/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.3843 - mae: 0.8578 - val_loss: 1.5172 - val_mae: 0.9452\n",
      "Epoch 33/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.3646 - mae: 0.8580 - val_loss: 1.5110 - val_mae: 0.9672\n",
      "Epoch 34/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.3500 - mae: 0.8568 - val_loss: 1.5105 - val_mae: 0.9751\n",
      "Epoch 35/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.3578 - mae: 0.8641 - val_loss: 1.4912 - val_mae: 0.9638\n",
      "Epoch 36/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.3429 - mae: 0.8605 - val_loss: 1.4879 - val_mae: 0.9370\n",
      "Epoch 37/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.3315 - mae: 0.8599 - val_loss: 1.4714 - val_mae: 0.9465\n",
      "Epoch 38/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.3110 - mae: 0.8513 - val_loss: 1.4900 - val_mae: 0.9779\n",
      "Epoch 39/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.3128 - mae: 0.8576 - val_loss: 1.4636 - val_mae: 0.9496\n",
      "Epoch 40/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.3336 - mae: 0.8606 - val_loss: 1.4663 - val_mae: 0.9526\n",
      "Epoch 41/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.3015 - mae: 0.8545 - val_loss: 1.4618 - val_mae: 0.9619\n",
      "Epoch 42/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2997 - mae: 0.8564 - val_loss: 1.4621 - val_mae: 0.9591\n",
      "Epoch 43/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.3106 - mae: 0.8587 - val_loss: 1.4552 - val_mae: 0.9432\n",
      "Epoch 44/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2956 - mae: 0.8625 - val_loss: 1.4545 - val_mae: 0.9601\n",
      "Epoch 45/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2911 - mae: 0.8589 - val_loss: 1.4525 - val_mae: 0.9607\n",
      "Epoch 46/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2893 - mae: 0.8584 - val_loss: 1.4596 - val_mae: 0.9349\n",
      "Epoch 47/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.3000 - mae: 0.8605 - val_loss: 1.4462 - val_mae: 0.9523\n",
      "Epoch 48/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2835 - mae: 0.8605 - val_loss: 1.4416 - val_mae: 0.9361\n",
      "Epoch 49/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.3010 - mae: 0.8655 - val_loss: 1.4478 - val_mae: 0.9354\n",
      "Epoch 50/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.3103 - mae: 0.8599 - val_loss: 1.4544 - val_mae: 0.9816\n",
      "Epoch 51/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2863 - mae: 0.8621 - val_loss: 1.4351 - val_mae: 0.9613\n",
      "Epoch 52/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2872 - mae: 0.8643 - val_loss: 1.4313 - val_mae: 0.9530\n",
      "Epoch 53/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2731 - mae: 0.8587 - val_loss: 1.4291 - val_mae: 0.9570\n",
      "Epoch 54/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2800 - mae: 0.8607 - val_loss: 1.4331 - val_mae: 0.9373\n",
      "Epoch 55/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2824 - mae: 0.8634 - val_loss: 1.4480 - val_mae: 0.9281\n",
      "Epoch 56/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2865 - mae: 0.8575 - val_loss: 1.4342 - val_mae: 0.9701\n",
      "Epoch 57/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2819 - mae: 0.8664 - val_loss: 1.4263 - val_mae: 0.9508\n",
      "Epoch 58/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.2581 - mae: 0.8551 - val_loss: 1.4558 - val_mae: 0.9876\n",
      "Epoch 59/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2566 - mae: 0.8599 - val_loss: 1.4255 - val_mae: 0.9340\n",
      "Epoch 60/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2491 - mae: 0.8557 - val_loss: 1.4231 - val_mae: 0.9570\n",
      "Epoch 61/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2682 - mae: 0.8621 - val_loss: 1.4215 - val_mae: 0.9503\n",
      "Epoch 62/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2705 - mae: 0.8585 - val_loss: 1.4295 - val_mae: 0.9318\n",
      "Epoch 63/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2713 - mae: 0.8545 - val_loss: 1.4203 - val_mae: 0.9544\n",
      "Epoch 64/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.2694 - mae: 0.8658 - val_loss: 1.4285 - val_mae: 0.9708\n",
      "Epoch 65/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2684 - mae: 0.8536 - val_loss: 1.4320 - val_mae: 0.9760\n",
      "Epoch 66/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2719 - mae: 0.8585 - val_loss: 1.4238 - val_mae: 0.9646\n",
      "Epoch 67/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2574 - mae: 0.8600 - val_loss: 1.4180 - val_mae: 0.9527\n",
      "Epoch 68/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2634 - mae: 0.8598 - val_loss: 1.4281 - val_mae: 0.9328\n",
      "Epoch 69/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2733 - mae: 0.8601 - val_loss: 1.4182 - val_mae: 0.9487\n",
      "Epoch 70/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2555 - mae: 0.8618 - val_loss: 1.4168 - val_mae: 0.9574\n",
      "Epoch 71/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2613 - mae: 0.8602 - val_loss: 1.4151 - val_mae: 0.9440\n",
      "Epoch 72/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2458 - mae: 0.8494 - val_loss: 1.4234 - val_mae: 0.9679\n",
      "Epoch 73/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2516 - mae: 0.8641 - val_loss: 1.4171 - val_mae: 0.9589\n",
      "Epoch 74/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.2479 - mae: 0.8604 - val_loss: 1.4225 - val_mae: 0.9314\n",
      "Epoch 75/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2558 - mae: 0.8568 - val_loss: 1.4188 - val_mae: 0.9386\n",
      "Epoch 76/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2508 - mae: 0.8518 - val_loss: 1.4151 - val_mae: 0.9518\n",
      "Epoch 77/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2562 - mae: 0.8609 - val_loss: 1.4145 - val_mae: 0.9410\n",
      "Epoch 78/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2603 - mae: 0.8485 - val_loss: 1.4302 - val_mae: 0.9779\n",
      "Epoch 79/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2568 - mae: 0.8651 - val_loss: 1.4128 - val_mae: 0.9492\n",
      "Epoch 80/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2523 - mae: 0.8592 - val_loss: 1.4144 - val_mae: 0.9510\n",
      "Epoch 81/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2473 - mae: 0.8568 - val_loss: 1.4098 - val_mae: 0.9443\n",
      "Epoch 82/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2609 - mae: 0.8588 - val_loss: 1.4396 - val_mae: 0.9852\n",
      "Epoch 83/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2574 - mae: 0.8603 - val_loss: 1.4179 - val_mae: 0.9588\n",
      "Epoch 84/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2502 - mae: 0.8676 - val_loss: 1.4134 - val_mae: 0.9539\n",
      "Epoch 85/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2489 - mae: 0.8588 - val_loss: 1.4161 - val_mae: 0.9609\n",
      "Epoch 86/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2685 - mae: 0.8595 - val_loss: 1.4152 - val_mae: 0.9563\n",
      "Epoch 87/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2533 - mae: 0.8632 - val_loss: 1.4160 - val_mae: 0.9440\n",
      "Epoch 88/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2657 - mae: 0.8632 - val_loss: 1.4177 - val_mae: 0.9526\n",
      "Epoch 89/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2598 - mae: 0.8639 - val_loss: 1.4131 - val_mae: 0.9561\n",
      "Epoch 90/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2620 - mae: 0.8679 - val_loss: 1.4183 - val_mae: 0.9388\n",
      "Epoch 91/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2616 - mae: 0.8530 - val_loss: 1.4098 - val_mae: 0.9471\n",
      "Epoch 92/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2496 - mae: 0.8573 - val_loss: 1.4087 - val_mae: 0.9413\n",
      "Epoch 93/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2473 - mae: 0.8587 - val_loss: 1.4085 - val_mae: 0.9487\n",
      "Epoch 94/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2542 - mae: 0.8563 - val_loss: 1.4204 - val_mae: 0.9725\n",
      "Epoch 95/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2713 - mae: 0.8721 - val_loss: 1.4109 - val_mae: 0.9547\n",
      "Epoch 96/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2541 - mae: 0.8639 - val_loss: 1.4151 - val_mae: 0.9468\n",
      "Epoch 97/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2526 - mae: 0.8529 - val_loss: 1.4101 - val_mae: 0.9556\n",
      "Epoch 98/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2506 - mae: 0.8623 - val_loss: 1.4180 - val_mae: 0.9320\n",
      "Epoch 99/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2404 - mae: 0.8601 - val_loss: 1.4067 - val_mae: 0.9412\n",
      "Epoch 100/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2577 - mae: 0.8587 - val_loss: 1.4144 - val_mae: 0.9617\n",
      "Epoch 101/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2684 - mae: 0.8718 - val_loss: 1.4343 - val_mae: 0.9274\n",
      "Epoch 102/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.2561 - mae: 0.8568 - val_loss: 1.4118 - val_mae: 0.9456\n",
      "Epoch 103/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2611 - mae: 0.8654 - val_loss: 1.4212 - val_mae: 0.9701\n",
      "Epoch 104/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2506 - mae: 0.8656 - val_loss: 1.4224 - val_mae: 0.9725\n",
      "Epoch 105/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2596 - mae: 0.8661 - val_loss: 1.4145 - val_mae: 0.9328\n",
      "Epoch 106/3000\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.2663 - mae: 0.8615 - val_loss: 1.4138 - val_mae: 0.9553\n",
      "Epoch 107/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2665 - mae: 0.8654 - val_loss: 1.4128 - val_mae: 0.9581\n",
      "Epoch 108/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2538 - mae: 0.8608 - val_loss: 1.4238 - val_mae: 0.9762\n",
      "Epoch 109/3000\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.2551 - mae: 0.8629 - val_loss: 1.4202 - val_mae: 0.9366\n",
      "Epoch 110/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2466 - mae: 0.8572 - val_loss: 1.4095 - val_mae: 0.9429\n",
      "Epoch 111/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2531 - mae: 0.8617 - val_loss: 1.4294 - val_mae: 0.9309\n",
      "Epoch 112/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2520 - mae: 0.8670 - val_loss: 1.4198 - val_mae: 0.9347\n",
      "Epoch 113/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2483 - mae: 0.8596 - val_loss: 1.4145 - val_mae: 0.9431\n",
      "Epoch 114/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2618 - mae: 0.8648 - val_loss: 1.4351 - val_mae: 0.9305\n",
      "Epoch 115/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2547 - mae: 0.8596 - val_loss: 1.4187 - val_mae: 0.9331\n",
      "Epoch 116/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2469 - mae: 0.8593 - val_loss: 1.4116 - val_mae: 0.9563\n",
      "Epoch 117/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2465 - mae: 0.8553 - val_loss: 1.4125 - val_mae: 0.9591\n",
      "Epoch 118/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2676 - mae: 0.8703 - val_loss: 1.4198 - val_mae: 0.9400\n",
      "Epoch 119/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2675 - mae: 0.8635 - val_loss: 1.4157 - val_mae: 0.9354\n",
      "Epoch 120/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2661 - mae: 0.8696 - val_loss: 1.4286 - val_mae: 0.9345\n",
      "Epoch 121/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2535 - mae: 0.8632 - val_loss: 1.4060 - val_mae: 0.9441\n",
      "Epoch 122/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2514 - mae: 0.8551 - val_loss: 1.4236 - val_mae: 0.9776\n",
      "Epoch 123/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2672 - mae: 0.8746 - val_loss: 1.4118 - val_mae: 0.9604\n",
      "Epoch 124/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2546 - mae: 0.8600 - val_loss: 1.4166 - val_mae: 0.9692\n",
      "Epoch 125/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2511 - mae: 0.8566 - val_loss: 1.4125 - val_mae: 0.9464\n",
      "Epoch 126/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2587 - mae: 0.8623 - val_loss: 1.4087 - val_mae: 0.9511\n",
      "Epoch 127/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2593 - mae: 0.8684 - val_loss: 1.4128 - val_mae: 0.9533\n",
      "Epoch 128/3000\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.2499 - mae: 0.8595 - val_loss: 1.4107 - val_mae: 0.9482\n",
      "Epoch 129/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2502 - mae: 0.8542 - val_loss: 1.4100 - val_mae: 0.9371\n",
      "Epoch 130/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2477 - mae: 0.8599 - val_loss: 1.4087 - val_mae: 0.9397\n",
      "Epoch 131/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2435 - mae: 0.8579 - val_loss: 1.4178 - val_mae: 0.9306\n",
      "Epoch 132/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2571 - mae: 0.8636 - val_loss: 1.4126 - val_mae: 0.9439\n",
      "Epoch 133/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2524 - mae: 0.8579 - val_loss: 1.4233 - val_mae: 0.9357\n",
      "Epoch 134/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2499 - mae: 0.8609 - val_loss: 1.4126 - val_mae: 0.9581\n",
      "Epoch 135/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2635 - mae: 0.8653 - val_loss: 1.4194 - val_mae: 0.9315\n",
      "Epoch 136/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2560 - mae: 0.8578 - val_loss: 1.4078 - val_mae: 0.9537\n",
      "Epoch 137/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2576 - mae: 0.8602 - val_loss: 1.4188 - val_mae: 0.9603\n",
      "Epoch 138/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2508 - mae: 0.8620 - val_loss: 1.4132 - val_mae: 0.9432\n",
      "Epoch 139/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2570 - mae: 0.8636 - val_loss: 1.4089 - val_mae: 0.9560\n",
      "Epoch 140/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2487 - mae: 0.8572 - val_loss: 1.4115 - val_mae: 0.9575\n",
      "Epoch 141/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2601 - mae: 0.8624 - val_loss: 1.4190 - val_mae: 0.9579\n",
      "Epoch 142/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2555 - mae: 0.8577 - val_loss: 1.4092 - val_mae: 0.9427\n",
      "Epoch 143/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2601 - mae: 0.8564 - val_loss: 1.4110 - val_mae: 0.9511\n",
      "Epoch 144/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2459 - mae: 0.8611 - val_loss: 1.4136 - val_mae: 0.9313\n",
      "Epoch 145/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2524 - mae: 0.8546 - val_loss: 1.4093 - val_mae: 0.9396\n",
      "Epoch 146/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2449 - mae: 0.8570 - val_loss: 1.4105 - val_mae: 0.9382\n",
      "Epoch 147/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2664 - mae: 0.8634 - val_loss: 1.4308 - val_mae: 0.9844\n",
      "Epoch 148/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2495 - mae: 0.8624 - val_loss: 1.4098 - val_mae: 0.9549\n",
      "Epoch 149/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2637 - mae: 0.8624 - val_loss: 1.4146 - val_mae: 0.9629\n",
      "Epoch 150/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2546 - mae: 0.8559 - val_loss: 1.4134 - val_mae: 0.9660\n",
      "Epoch 151/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2511 - mae: 0.8606 - val_loss: 1.4067 - val_mae: 0.9539\n",
      "Epoch 152/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2661 - mae: 0.8681 - val_loss: 1.4144 - val_mae: 0.9508\n",
      "Epoch 153/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2637 - mae: 0.8641 - val_loss: 1.4080 - val_mae: 0.9440\n",
      "Epoch 154/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2330 - mae: 0.8555 - val_loss: 1.4103 - val_mae: 0.9523\n",
      "Epoch 155/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2572 - mae: 0.8632 - val_loss: 1.4197 - val_mae: 0.9353\n",
      "Epoch 156/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2516 - mae: 0.8531 - val_loss: 1.4237 - val_mae: 0.9736\n",
      "Epoch 157/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2512 - mae: 0.8580 - val_loss: 1.4366 - val_mae: 0.9885\n",
      "Epoch 158/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2505 - mae: 0.8648 - val_loss: 1.4123 - val_mae: 0.9479\n",
      "Epoch 159/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.2704 - mae: 0.8694 - val_loss: 1.4173 - val_mae: 0.9411\n",
      "Epoch 160/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2602 - mae: 0.8605 - val_loss: 1.4197 - val_mae: 0.9332\n",
      "Epoch 161/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2645 - mae: 0.8645 - val_loss: 1.4294 - val_mae: 0.9360\n",
      "Epoch 162/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2623 - mae: 0.8641 - val_loss: 1.4094 - val_mae: 0.9443\n",
      "Epoch 163/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2537 - mae: 0.8655 - val_loss: 1.4129 - val_mae: 0.9635\n",
      "Epoch 164/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2693 - mae: 0.8599 - val_loss: 1.4131 - val_mae: 0.9638\n",
      "Epoch 165/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2596 - mae: 0.8611 - val_loss: 1.4132 - val_mae: 0.9588\n",
      "Epoch 166/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2616 - mae: 0.8559 - val_loss: 1.4202 - val_mae: 0.9726\n",
      "Epoch 167/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2640 - mae: 0.8673 - val_loss: 1.4168 - val_mae: 0.9671\n",
      "Epoch 168/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2560 - mae: 0.8674 - val_loss: 1.4188 - val_mae: 0.9296\n",
      "Epoch 169/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2643 - mae: 0.8649 - val_loss: 1.4137 - val_mae: 0.9591\n",
      "Epoch 170/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2510 - mae: 0.8593 - val_loss: 1.4077 - val_mae: 0.9513\n",
      "Epoch 171/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2424 - mae: 0.8570 - val_loss: 1.4111 - val_mae: 0.9570\n",
      "Epoch 172/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2455 - mae: 0.8519 - val_loss: 1.4286 - val_mae: 0.9800\n",
      "Epoch 173/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2634 - mae: 0.8665 - val_loss: 1.4171 - val_mae: 0.9413\n",
      "Epoch 174/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2590 - mae: 0.8632 - val_loss: 1.4170 - val_mae: 0.9630\n",
      "Epoch 175/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2596 - mae: 0.8643 - val_loss: 1.4154 - val_mae: 0.9422\n",
      "Epoch 176/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2622 - mae: 0.8622 - val_loss: 1.4181 - val_mae: 0.9522\n",
      "Epoch 177/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2446 - mae: 0.8621 - val_loss: 1.4304 - val_mae: 0.9285\n",
      "Epoch 178/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2637 - mae: 0.8594 - val_loss: 1.4161 - val_mae: 0.9639\n",
      "Epoch 179/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2593 - mae: 0.8608 - val_loss: 1.4118 - val_mae: 0.9461\n",
      "Epoch 180/3000\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.2680 - mae: 0.8655 - val_loss: 1.4218 - val_mae: 0.9711\n",
      "Epoch 181/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2464 - mae: 0.8588 - val_loss: 1.4186 - val_mae: 0.9683\n",
      "Epoch 182/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2622 - mae: 0.8618 - val_loss: 1.4323 - val_mae: 0.9853\n",
      "Epoch 183/3000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2518 - mae: 0.8607 - val_loss: 1.4079 - val_mae: 0.9462\n",
      "Epoch 184/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2479 - mae: 0.8591 - val_loss: 1.4173 - val_mae: 0.9636\n",
      "Epoch 185/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2387 - mae: 0.8564 - val_loss: 1.4529 - val_mae: 0.9245\n",
      "Epoch 186/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2683 - mae: 0.8636 - val_loss: 1.4201 - val_mae: 0.9412\n",
      "Epoch 187/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2532 - mae: 0.8538 - val_loss: 1.4107 - val_mae: 0.9515\n",
      "Epoch 188/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2512 - mae: 0.8548 - val_loss: 1.4088 - val_mae: 0.9476\n",
      "Epoch 189/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2590 - mae: 0.8638 - val_loss: 1.4321 - val_mae: 0.9842\n",
      "Epoch 190/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2686 - mae: 0.8742 - val_loss: 1.4151 - val_mae: 0.9426\n",
      "Epoch 191/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2731 - mae: 0.8699 - val_loss: 1.4239 - val_mae: 0.9759\n",
      "Epoch 192/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2529 - mae: 0.8738 - val_loss: 1.4172 - val_mae: 0.9316\n",
      "Epoch 193/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2590 - mae: 0.8596 - val_loss: 1.4137 - val_mae: 0.9509\n",
      "Epoch 194/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2608 - mae: 0.8664 - val_loss: 1.4121 - val_mae: 0.9471\n",
      "Epoch 195/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2496 - mae: 0.8587 - val_loss: 1.4130 - val_mae: 0.9565\n",
      "Epoch 196/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2611 - mae: 0.8625 - val_loss: 1.4198 - val_mae: 0.9687\n",
      "Epoch 197/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2468 - mae: 0.8554 - val_loss: 1.4243 - val_mae: 0.9754\n",
      "Epoch 198/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2702 - mae: 0.8705 - val_loss: 1.4162 - val_mae: 0.9664\n",
      "Epoch 199/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2615 - mae: 0.8666 - val_loss: 1.4376 - val_mae: 0.9887\n",
      "Epoch 200/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2557 - mae: 0.8648 - val_loss: 1.4121 - val_mae: 0.9592\n",
      "Epoch 201/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2535 - mae: 0.8579 - val_loss: 1.4226 - val_mae: 0.9679\n",
      "Epoch 202/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2535 - mae: 0.8634 - val_loss: 1.4136 - val_mae: 0.9558\n",
      "Epoch 203/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2537 - mae: 0.8652 - val_loss: 1.4171 - val_mae: 0.9592\n",
      "Epoch 204/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2474 - mae: 0.8614 - val_loss: 1.4296 - val_mae: 0.9400\n",
      "Epoch 205/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2669 - mae: 0.8591 - val_loss: 1.4135 - val_mae: 0.9530\n",
      "Epoch 206/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2602 - mae: 0.8662 - val_loss: 1.4146 - val_mae: 0.9607\n",
      "Epoch 207/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2471 - mae: 0.8530 - val_loss: 1.4140 - val_mae: 0.9609\n",
      "Epoch 208/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2498 - mae: 0.8595 - val_loss: 1.4166 - val_mae: 0.9520\n",
      "Epoch 209/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2606 - mae: 0.8592 - val_loss: 1.4102 - val_mae: 0.9548\n",
      "Epoch 210/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2691 - mae: 0.8585 - val_loss: 1.4165 - val_mae: 0.9627\n",
      "Epoch 211/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2558 - mae: 0.8611 - val_loss: 1.4161 - val_mae: 0.9660\n",
      "Epoch 212/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2478 - mae: 0.8577 - val_loss: 1.4317 - val_mae: 0.9285\n",
      "Epoch 213/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2726 - mae: 0.8598 - val_loss: 1.4245 - val_mae: 0.9516\n",
      "Epoch 214/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2505 - mae: 0.8687 - val_loss: 1.4232 - val_mae: 0.9738\n",
      "Epoch 215/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2685 - mae: 0.8699 - val_loss: 1.4127 - val_mae: 0.9419\n",
      "Epoch 216/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2536 - mae: 0.8579 - val_loss: 1.4084 - val_mae: 0.9544\n",
      "Epoch 217/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2503 - mae: 0.8567 - val_loss: 1.4080 - val_mae: 0.9543\n",
      "Epoch 218/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2438 - mae: 0.8552 - val_loss: 1.4093 - val_mae: 0.9538\n",
      "Epoch 219/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2699 - mae: 0.8704 - val_loss: 1.4272 - val_mae: 0.9807\n",
      "Epoch 220/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2506 - mae: 0.8628 - val_loss: 1.4115 - val_mae: 0.9405\n",
      "Epoch 221/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.2525 - mae: 0.8586 - val_loss: 1.4164 - val_mae: 0.9413\n",
      "Epoch 1/3000\n",
      "36/36 [==============================] - 3s 16ms/step - loss: 4.8637 - mae: 1.0962 - val_loss: 4.3590 - val_mae: 0.9756\n",
      "Epoch 2/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 4.4190 - mae: 1.0425 - val_loss: 4.0624 - val_mae: 0.9837\n",
      "Epoch 3/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 4.0961 - mae: 1.0267 - val_loss: 3.9839 - val_mae: 0.9852\n",
      "Epoch 4/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 3.7844 - mae: 0.9922 - val_loss: 3.6203 - val_mae: 0.9906\n",
      "Epoch 5/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.6188 - mae: 1.0170 - val_loss: 3.4322 - val_mae: 0.9831\n",
      "Epoch 6/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 3.3751 - mae: 0.9962 - val_loss: 3.2810 - val_mae: 1.0145\n",
      "Epoch 7/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 3.2004 - mae: 0.9983 - val_loss: 3.1066 - val_mae: 0.9802\n",
      "Epoch 8/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 3.0655 - mae: 0.9933 - val_loss: 2.9883 - val_mae: 1.0064\n",
      "Epoch 9/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.9604 - mae: 0.9941 - val_loss: 2.9107 - val_mae: 1.0386\n",
      "Epoch 10/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 2.7853 - mae: 0.9910 - val_loss: 2.7582 - val_mae: 1.0081\n",
      "Epoch 11/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.6745 - mae: 0.9780 - val_loss: 2.6454 - val_mae: 0.9839\n",
      "Epoch 12/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.6019 - mae: 0.9842 - val_loss: 2.5641 - val_mae: 0.9915\n",
      "Epoch 13/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.5270 - mae: 0.9971 - val_loss: 2.5047 - val_mae: 0.9734\n",
      "Epoch 14/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.4292 - mae: 0.9839 - val_loss: 2.4178 - val_mae: 0.9995\n",
      "Epoch 15/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.3432 - mae: 0.9848 - val_loss: 2.3544 - val_mae: 0.9818\n",
      "Epoch 16/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.2908 - mae: 0.9756 - val_loss: 2.3219 - val_mae: 1.0205\n",
      "Epoch 17/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 2.2494 - mae: 0.9904 - val_loss: 2.2429 - val_mae: 0.9915\n",
      "Epoch 18/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.2247 - mae: 0.9892 - val_loss: 2.2276 - val_mae: 0.9761\n",
      "Epoch 19/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.1669 - mae: 0.9831 - val_loss: 2.1718 - val_mae: 1.0189\n",
      "Epoch 20/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.1258 - mae: 0.9892 - val_loss: 2.1307 - val_mae: 1.0170\n",
      "Epoch 21/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.0683 - mae: 0.9798 - val_loss: 2.0749 - val_mae: 0.9928\n",
      "Epoch 22/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 2.0133 - mae: 0.9769 - val_loss: 2.0455 - val_mae: 1.0008\n",
      "Epoch 23/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.0106 - mae: 0.9853 - val_loss: 2.0151 - val_mae: 0.9910\n",
      "Epoch 24/3000\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 1.9689 - mae: 0.9841 - val_loss: 1.9817 - val_mae: 0.9912\n",
      "Epoch 25/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.9771 - mae: 0.9900 - val_loss: 1.9595 - val_mae: 0.9838\n",
      "Epoch 26/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.9015 - mae: 0.9723 - val_loss: 1.9591 - val_mae: 1.0148\n",
      "Epoch 27/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.8541 - mae: 0.9639 - val_loss: 1.9170 - val_mae: 0.9881\n",
      "Epoch 28/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.8672 - mae: 0.9743 - val_loss: 1.9007 - val_mae: 0.9962\n",
      "Epoch 29/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.8500 - mae: 0.9800 - val_loss: 1.9286 - val_mae: 0.9741\n",
      "Epoch 30/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.8889 - mae: 0.9904 - val_loss: 1.8644 - val_mae: 0.9835\n",
      "Epoch 31/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.7964 - mae: 0.9774 - val_loss: 1.8833 - val_mae: 1.0256\n",
      "Epoch 32/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.8063 - mae: 0.9815 - val_loss: 1.8380 - val_mae: 0.9952\n",
      "Epoch 33/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.8176 - mae: 0.9919 - val_loss: 1.8568 - val_mae: 0.9751\n",
      "Epoch 34/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.7934 - mae: 0.9765 - val_loss: 1.8106 - val_mae: 0.9871\n",
      "Epoch 35/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.7757 - mae: 0.9828 - val_loss: 1.7986 - val_mae: 0.9870\n",
      "Epoch 36/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.7432 - mae: 0.9704 - val_loss: 1.7925 - val_mae: 0.9772\n",
      "Epoch 37/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.7761 - mae: 0.9855 - val_loss: 1.7798 - val_mae: 0.9948\n",
      "Epoch 38/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.7273 - mae: 0.9809 - val_loss: 1.7735 - val_mae: 0.9795\n",
      "Epoch 39/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.7237 - mae: 0.9766 - val_loss: 1.7816 - val_mae: 1.0139\n",
      "Epoch 40/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.7180 - mae: 0.9800 - val_loss: 1.7641 - val_mae: 1.0030\n",
      "Epoch 41/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.7218 - mae: 0.9824 - val_loss: 1.7657 - val_mae: 0.9781\n",
      "Epoch 42/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.6960 - mae: 0.9771 - val_loss: 1.7486 - val_mae: 1.0029\n",
      "Epoch 43/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.7244 - mae: 0.9918 - val_loss: 1.7454 - val_mae: 0.9767\n",
      "Epoch 44/3000\n",
      "36/36 [==============================] - 1s 21ms/step - loss: 1.7125 - mae: 0.9888 - val_loss: 1.7672 - val_mae: 0.9805\n",
      "Epoch 45/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6720 - mae: 0.9826 - val_loss: 1.7205 - val_mae: 0.9892\n",
      "Epoch 46/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.6916 - mae: 0.9825 - val_loss: 1.7178 - val_mae: 0.9805\n",
      "Epoch 47/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6816 - mae: 0.9791 - val_loss: 1.7162 - val_mae: 0.9803\n",
      "Epoch 48/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6702 - mae: 0.9742 - val_loss: 1.7402 - val_mae: 1.0209\n",
      "Epoch 49/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.6726 - mae: 0.9836 - val_loss: 1.7074 - val_mae: 0.9849\n",
      "Epoch 50/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6781 - mae: 0.9843 - val_loss: 1.7089 - val_mae: 0.9789\n",
      "Epoch 51/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.6527 - mae: 0.9748 - val_loss: 1.7128 - val_mae: 1.0108\n",
      "Epoch 52/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6585 - mae: 0.9775 - val_loss: 1.6904 - val_mae: 0.9837\n",
      "Epoch 53/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6290 - mae: 0.9752 - val_loss: 1.6924 - val_mae: 0.9788\n",
      "Epoch 54/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6421 - mae: 0.9797 - val_loss: 1.6873 - val_mae: 0.9794\n",
      "Epoch 55/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.6491 - mae: 0.9791 - val_loss: 1.6927 - val_mae: 0.9756\n",
      "Epoch 56/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6383 - mae: 0.9778 - val_loss: 1.6848 - val_mae: 0.9808\n",
      "Epoch 57/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6276 - mae: 0.9694 - val_loss: 1.7179 - val_mae: 1.0274\n",
      "Epoch 58/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6539 - mae: 0.9870 - val_loss: 1.6882 - val_mae: 1.0033\n",
      "Epoch 59/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6122 - mae: 0.9745 - val_loss: 1.6762 - val_mae: 0.9847\n",
      "Epoch 60/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6182 - mae: 0.9744 - val_loss: 1.6821 - val_mae: 1.0033\n",
      "Epoch 61/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6181 - mae: 0.9782 - val_loss: 1.6745 - val_mae: 0.9808\n",
      "Epoch 62/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.6382 - mae: 0.9834 - val_loss: 1.6770 - val_mae: 1.0016\n",
      "Epoch 63/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6072 - mae: 0.9709 - val_loss: 1.6663 - val_mae: 0.9790\n",
      "Epoch 64/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6084 - mae: 0.9746 - val_loss: 1.6744 - val_mae: 0.9800\n",
      "Epoch 65/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6306 - mae: 0.9783 - val_loss: 1.6892 - val_mae: 1.0204\n",
      "Epoch 66/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6210 - mae: 0.9853 - val_loss: 1.6651 - val_mae: 0.9909\n",
      "Epoch 67/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6277 - mae: 0.9760 - val_loss: 1.6578 - val_mae: 0.9869\n",
      "Epoch 68/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.6139 - mae: 0.9824 - val_loss: 1.6562 - val_mae: 0.9879\n",
      "Epoch 69/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6067 - mae: 0.9753 - val_loss: 1.6651 - val_mae: 0.9850\n",
      "Epoch 70/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6115 - mae: 0.9728 - val_loss: 1.6747 - val_mae: 1.0054\n",
      "Epoch 71/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6300 - mae: 0.9852 - val_loss: 1.6656 - val_mae: 0.9836\n",
      "Epoch 72/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6087 - mae: 0.9824 - val_loss: 1.6513 - val_mae: 0.9875\n",
      "Epoch 73/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6044 - mae: 0.9779 - val_loss: 1.6494 - val_mae: 0.9889\n",
      "Epoch 74/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5902 - mae: 0.9752 - val_loss: 1.6500 - val_mae: 0.9831\n",
      "Epoch 75/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6102 - mae: 0.9859 - val_loss: 1.6707 - val_mae: 0.9735\n",
      "Epoch 76/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5869 - mae: 0.9674 - val_loss: 1.7266 - val_mae: 1.0517\n",
      "Epoch 77/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5861 - mae: 0.9759 - val_loss: 1.6530 - val_mae: 0.9822\n",
      "Epoch 78/3000\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 1.5982 - mae: 0.9757 - val_loss: 1.6553 - val_mae: 0.9986\n",
      "Epoch 79/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5857 - mae: 0.9727 - val_loss: 1.6481 - val_mae: 0.9834\n",
      "Epoch 80/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6154 - mae: 0.9783 - val_loss: 1.6498 - val_mae: 0.9791\n",
      "Epoch 81/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.6155 - mae: 0.9866 - val_loss: 1.6527 - val_mae: 0.9761\n",
      "Epoch 82/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5787 - mae: 0.9749 - val_loss: 1.6460 - val_mae: 0.9840\n",
      "Epoch 83/3000\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.6075 - mae: 0.9760 - val_loss: 1.6478 - val_mae: 1.0010\n",
      "Epoch 84/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5862 - mae: 0.9798 - val_loss: 1.6612 - val_mae: 1.0045\n",
      "Epoch 85/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5835 - mae: 0.9739 - val_loss: 1.6467 - val_mae: 0.9976\n",
      "Epoch 86/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5886 - mae: 0.9766 - val_loss: 1.6519 - val_mae: 1.0036\n",
      "Epoch 87/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5985 - mae: 0.9822 - val_loss: 1.6498 - val_mae: 1.0031\n",
      "Epoch 88/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6039 - mae: 0.9769 - val_loss: 1.6411 - val_mae: 0.9878\n",
      "Epoch 89/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6034 - mae: 0.9806 - val_loss: 1.6428 - val_mae: 0.9984\n",
      "Epoch 90/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5928 - mae: 0.9725 - val_loss: 1.6588 - val_mae: 1.0123\n",
      "Epoch 91/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6079 - mae: 0.9814 - val_loss: 1.6565 - val_mae: 1.0106\n",
      "Epoch 92/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5996 - mae: 0.9830 - val_loss: 1.6500 - val_mae: 0.9993\n",
      "Epoch 93/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5744 - mae: 0.9693 - val_loss: 1.6958 - val_mae: 1.0416\n",
      "Epoch 94/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5876 - mae: 0.9828 - val_loss: 1.6593 - val_mae: 1.0137\n",
      "Epoch 95/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5851 - mae: 0.9776 - val_loss: 1.6394 - val_mae: 0.9960\n",
      "Epoch 96/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5800 - mae: 0.9725 - val_loss: 1.6400 - val_mae: 0.9917\n",
      "Epoch 97/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5867 - mae: 0.9816 - val_loss: 1.6462 - val_mae: 1.0012\n",
      "Epoch 98/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5691 - mae: 0.9706 - val_loss: 1.6407 - val_mae: 0.9811\n",
      "Epoch 99/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5813 - mae: 0.9753 - val_loss: 1.6420 - val_mae: 0.9793\n",
      "Epoch 100/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6024 - mae: 0.9726 - val_loss: 1.6380 - val_mae: 0.9883\n",
      "Epoch 101/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6129 - mae: 0.9933 - val_loss: 1.6356 - val_mae: 0.9866\n",
      "Epoch 102/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5739 - mae: 0.9716 - val_loss: 1.6383 - val_mae: 0.9810\n",
      "Epoch 103/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5737 - mae: 0.9690 - val_loss: 1.6419 - val_mae: 0.9965\n",
      "Epoch 104/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5622 - mae: 0.9678 - val_loss: 1.6353 - val_mae: 0.9835\n",
      "Epoch 105/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.5524 - mae: 0.9674 - val_loss: 1.6866 - val_mae: 1.0303\n",
      "Epoch 106/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5967 - mae: 0.9826 - val_loss: 1.6502 - val_mae: 0.9759\n",
      "Epoch 107/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5787 - mae: 0.9779 - val_loss: 1.6479 - val_mae: 0.9750\n",
      "Epoch 108/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6031 - mae: 0.9757 - val_loss: 1.6396 - val_mae: 0.9822\n",
      "Epoch 109/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5845 - mae: 0.9722 - val_loss: 1.6697 - val_mae: 1.0237\n",
      "Epoch 110/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5975 - mae: 0.9839 - val_loss: 1.6358 - val_mae: 0.9944\n",
      "Epoch 111/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5785 - mae: 0.9752 - val_loss: 1.6541 - val_mae: 1.0120\n",
      "Epoch 112/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5815 - mae: 0.9741 - val_loss: 1.6363 - val_mae: 0.9945\n",
      "Epoch 113/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5916 - mae: 0.9807 - val_loss: 1.6375 - val_mae: 0.9912\n",
      "Epoch 114/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5744 - mae: 0.9794 - val_loss: 1.6576 - val_mae: 1.0142\n",
      "Epoch 115/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5675 - mae: 0.9746 - val_loss: 1.6389 - val_mae: 0.9905\n",
      "Epoch 116/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.6158 - mae: 0.9848 - val_loss: 1.6502 - val_mae: 0.9735\n",
      "Epoch 117/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5688 - mae: 0.9794 - val_loss: 1.6680 - val_mae: 0.9744\n",
      "Epoch 118/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5942 - mae: 0.9819 - val_loss: 1.6376 - val_mae: 0.9783\n",
      "Epoch 119/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5764 - mae: 0.9715 - val_loss: 1.6339 - val_mae: 0.9923\n",
      "Epoch 120/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5785 - mae: 0.9785 - val_loss: 1.6479 - val_mae: 0.9753\n",
      "Epoch 121/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5749 - mae: 0.9659 - val_loss: 1.6375 - val_mae: 0.9973\n",
      "Epoch 122/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.5570 - mae: 0.9648 - val_loss: 1.6404 - val_mae: 0.9784\n",
      "Epoch 123/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5976 - mae: 0.9829 - val_loss: 1.6857 - val_mae: 1.0321\n",
      "Epoch 124/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6027 - mae: 0.9829 - val_loss: 1.6887 - val_mae: 1.0334\n",
      "Epoch 125/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5896 - mae: 0.9795 - val_loss: 1.6978 - val_mae: 1.0461\n",
      "Epoch 126/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5769 - mae: 0.9826 - val_loss: 1.6581 - val_mae: 1.0150\n",
      "Epoch 127/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5867 - mae: 0.9808 - val_loss: 1.6332 - val_mae: 0.9877\n",
      "Epoch 128/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.6142 - mae: 0.9886 - val_loss: 1.6315 - val_mae: 0.9920\n",
      "Epoch 129/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5684 - mae: 0.9733 - val_loss: 1.6449 - val_mae: 1.0058\n",
      "Epoch 130/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5834 - mae: 0.9749 - val_loss: 1.6485 - val_mae: 1.0048\n",
      "Epoch 131/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5975 - mae: 0.9855 - val_loss: 1.6563 - val_mae: 1.0176\n",
      "Epoch 132/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5718 - mae: 0.9718 - val_loss: 1.6714 - val_mae: 0.9737\n",
      "Epoch 133/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5885 - mae: 0.9789 - val_loss: 1.6438 - val_mae: 1.0013\n",
      "Epoch 134/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5745 - mae: 0.9776 - val_loss: 1.6470 - val_mae: 1.0041\n",
      "Epoch 135/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5914 - mae: 0.9806 - val_loss: 1.6602 - val_mae: 1.0210\n",
      "Epoch 136/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6012 - mae: 0.9869 - val_loss: 1.6339 - val_mae: 0.9938\n",
      "Epoch 137/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5951 - mae: 0.9837 - val_loss: 1.6797 - val_mae: 1.0326\n",
      "Epoch 138/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5809 - mae: 0.9767 - val_loss: 1.6346 - val_mae: 0.9958\n",
      "Epoch 139/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.5938 - mae: 0.9810 - val_loss: 1.6437 - val_mae: 1.0095\n",
      "Epoch 140/3000\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 1.5770 - mae: 0.9766 - val_loss: 1.6511 - val_mae: 1.0142\n",
      "Epoch 141/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5790 - mae: 0.9783 - val_loss: 1.6447 - val_mae: 1.0086\n",
      "Epoch 142/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5725 - mae: 0.9794 - val_loss: 1.6327 - val_mae: 0.9862\n",
      "Epoch 143/3000\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.5767 - mae: 0.9786 - val_loss: 1.6488 - val_mae: 0.9749\n",
      "Epoch 144/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5785 - mae: 0.9809 - val_loss: 1.6365 - val_mae: 0.9812\n",
      "Epoch 145/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5800 - mae: 0.9779 - val_loss: 1.6351 - val_mae: 0.9863\n",
      "Epoch 146/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5773 - mae: 0.9756 - val_loss: 1.6405 - val_mae: 0.9998\n",
      "Epoch 147/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5888 - mae: 0.9744 - val_loss: 1.6516 - val_mae: 1.0106\n",
      "Epoch 148/3000\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.5745 - mae: 0.9758 - val_loss: 1.6357 - val_mae: 0.9940\n",
      "Epoch 149/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5765 - mae: 0.9723 - val_loss: 1.6498 - val_mae: 0.9751\n",
      "Epoch 150/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6071 - mae: 0.9888 - val_loss: 1.6469 - val_mae: 0.9753\n",
      "Epoch 151/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5835 - mae: 0.9755 - val_loss: 1.6402 - val_mae: 1.0014\n",
      "Epoch 152/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5800 - mae: 0.9761 - val_loss: 1.6512 - val_mae: 0.9749\n",
      "Epoch 153/3000\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.6059 - mae: 0.9877 - val_loss: 1.6502 - val_mae: 0.9835\n",
      "Epoch 154/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5890 - mae: 0.9780 - val_loss: 1.6360 - val_mae: 0.9848\n",
      "Epoch 155/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6004 - mae: 0.9812 - val_loss: 1.6337 - val_mae: 0.9867\n",
      "Epoch 156/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5745 - mae: 0.9735 - val_loss: 1.6812 - val_mae: 1.0337\n",
      "Epoch 157/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5763 - mae: 0.9757 - val_loss: 1.6414 - val_mae: 0.9779\n",
      "Epoch 158/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5731 - mae: 0.9699 - val_loss: 1.6485 - val_mae: 0.9775\n",
      "Epoch 159/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5844 - mae: 0.9720 - val_loss: 1.6373 - val_mae: 0.9904\n",
      "Epoch 160/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5665 - mae: 0.9759 - val_loss: 1.6683 - val_mae: 1.0258\n",
      "Epoch 161/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5887 - mae: 0.9803 - val_loss: 1.6885 - val_mae: 1.0366\n",
      "Epoch 162/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5956 - mae: 0.9874 - val_loss: 1.6406 - val_mae: 0.9922\n",
      "Epoch 163/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5841 - mae: 0.9737 - val_loss: 1.6365 - val_mae: 0.9881\n",
      "Epoch 164/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5952 - mae: 0.9800 - val_loss: 1.6730 - val_mae: 1.0288\n",
      "Epoch 165/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5839 - mae: 0.9792 - val_loss: 1.6539 - val_mae: 1.0145\n",
      "Epoch 166/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5829 - mae: 0.9807 - val_loss: 1.6410 - val_mae: 0.9850\n",
      "Epoch 167/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5782 - mae: 0.9775 - val_loss: 1.6389 - val_mae: 1.0004\n",
      "Epoch 168/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5983 - mae: 0.9821 - val_loss: 1.6334 - val_mae: 0.9952\n",
      "Epoch 169/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5664 - mae: 0.9739 - val_loss: 1.6333 - val_mae: 0.9841\n",
      "Epoch 170/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.6215 - mae: 0.9884 - val_loss: 1.6435 - val_mae: 0.9855\n",
      "Epoch 171/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5864 - mae: 0.9865 - val_loss: 1.6394 - val_mae: 0.9896\n",
      "Epoch 172/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5747 - mae: 0.9731 - val_loss: 1.6393 - val_mae: 0.9942\n",
      "Epoch 173/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5782 - mae: 0.9773 - val_loss: 1.6464 - val_mae: 1.0089\n",
      "Epoch 174/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5851 - mae: 0.9860 - val_loss: 1.6406 - val_mae: 0.9982\n",
      "Epoch 175/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5859 - mae: 0.9794 - val_loss: 1.6495 - val_mae: 1.0107\n",
      "Epoch 176/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.6064 - mae: 0.9929 - val_loss: 1.6362 - val_mae: 0.9973\n",
      "Epoch 177/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5815 - mae: 0.9760 - val_loss: 1.6358 - val_mae: 0.9889\n",
      "Epoch 178/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5673 - mae: 0.9751 - val_loss: 1.6474 - val_mae: 0.9981\n",
      "Epoch 179/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6064 - mae: 0.9855 - val_loss: 1.6362 - val_mae: 0.9942\n",
      "Epoch 180/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5859 - mae: 0.9725 - val_loss: 1.6329 - val_mae: 0.9849\n",
      "Epoch 181/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5821 - mae: 0.9803 - val_loss: 1.6387 - val_mae: 0.9957\n",
      "Epoch 182/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5674 - mae: 0.9743 - val_loss: 1.6596 - val_mae: 1.0122\n",
      "Epoch 183/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5816 - mae: 0.9771 - val_loss: 1.6317 - val_mae: 0.9843\n",
      "Epoch 184/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5631 - mae: 0.9726 - val_loss: 1.6543 - val_mae: 1.0141\n",
      "Epoch 185/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6057 - mae: 0.9817 - val_loss: 1.6338 - val_mae: 0.9808\n",
      "Epoch 186/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.6047 - mae: 0.9906 - val_loss: 1.6440 - val_mae: 1.0064\n",
      "Epoch 187/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6028 - mae: 0.9884 - val_loss: 1.6736 - val_mae: 1.0256\n",
      "Epoch 188/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5923 - mae: 0.9839 - val_loss: 1.6401 - val_mae: 0.9920\n",
      "Epoch 189/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5747 - mae: 0.9737 - val_loss: 1.6540 - val_mae: 0.9944\n",
      "Epoch 190/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5916 - mae: 0.9776 - val_loss: 1.6543 - val_mae: 1.0137\n",
      "Epoch 191/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5824 - mae: 0.9712 - val_loss: 1.6610 - val_mae: 1.0189\n",
      "Epoch 192/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.6012 - mae: 0.9814 - val_loss: 1.6377 - val_mae: 0.9834\n",
      "Epoch 193/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5858 - mae: 0.9834 - val_loss: 1.6375 - val_mae: 0.9803\n",
      "Epoch 194/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5756 - mae: 0.9734 - val_loss: 1.6428 - val_mae: 0.9814\n",
      "Epoch 195/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5872 - mae: 0.9745 - val_loss: 1.6370 - val_mae: 0.9984\n",
      "Epoch 196/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5714 - mae: 0.9749 - val_loss: 1.6395 - val_mae: 0.9786\n",
      "Epoch 197/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.5819 - mae: 0.9775 - val_loss: 1.6517 - val_mae: 1.0080\n",
      "Epoch 198/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5768 - mae: 0.9759 - val_loss: 1.6393 - val_mae: 1.0011\n",
      "Epoch 199/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5865 - mae: 0.9806 - val_loss: 1.6461 - val_mae: 0.9753\n",
      "Epoch 200/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6050 - mae: 0.9795 - val_loss: 1.6554 - val_mae: 1.0172\n",
      "Epoch 201/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5636 - mae: 0.9794 - val_loss: 1.7318 - val_mae: 1.0579\n",
      "Epoch 202/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.6098 - mae: 0.9872 - val_loss: 1.6359 - val_mae: 0.9890\n",
      "Epoch 203/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.6060 - mae: 0.9831 - val_loss: 1.6491 - val_mae: 1.0112\n",
      "Epoch 204/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5758 - mae: 0.9752 - val_loss: 1.6577 - val_mae: 1.0179\n",
      "Epoch 205/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5817 - mae: 0.9812 - val_loss: 1.6468 - val_mae: 1.0077\n",
      "Epoch 206/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5897 - mae: 0.9799 - val_loss: 1.6338 - val_mae: 0.9910\n",
      "Epoch 207/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5739 - mae: 0.9751 - val_loss: 1.6394 - val_mae: 0.9804\n",
      "Epoch 208/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5738 - mae: 0.9793 - val_loss: 1.6352 - val_mae: 0.9882\n",
      "Epoch 209/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5755 - mae: 0.9711 - val_loss: 1.6345 - val_mae: 0.9863\n",
      "Epoch 210/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5892 - mae: 0.9809 - val_loss: 1.6327 - val_mae: 0.9922\n",
      "Epoch 211/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5874 - mae: 0.9841 - val_loss: 1.6524 - val_mae: 0.9749\n",
      "Epoch 212/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5692 - mae: 0.9763 - val_loss: 1.6362 - val_mae: 0.9796\n",
      "Epoch 213/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5699 - mae: 0.9789 - val_loss: 1.6396 - val_mae: 0.9936\n",
      "Epoch 214/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5867 - mae: 0.9839 - val_loss: 1.6598 - val_mae: 0.9729\n",
      "Epoch 215/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6007 - mae: 0.9814 - val_loss: 1.6385 - val_mae: 0.9792\n",
      "Epoch 216/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5848 - mae: 0.9755 - val_loss: 1.6739 - val_mae: 1.0295\n",
      "Epoch 217/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5894 - mae: 0.9821 - val_loss: 1.6404 - val_mae: 0.9782\n",
      "Epoch 218/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5886 - mae: 0.9716 - val_loss: 1.6329 - val_mae: 0.9888\n",
      "Epoch 219/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5810 - mae: 0.9776 - val_loss: 1.6490 - val_mae: 1.0112\n",
      "Epoch 220/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5772 - mae: 0.9796 - val_loss: 1.6368 - val_mae: 0.9784\n",
      "Epoch 221/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.6021 - mae: 0.9820 - val_loss: 1.6429 - val_mae: 0.9906\n",
      "Epoch 222/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5812 - mae: 0.9759 - val_loss: 1.6414 - val_mae: 0.9955\n",
      "Epoch 223/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5882 - mae: 0.9759 - val_loss: 1.6462 - val_mae: 0.9820\n",
      "Epoch 224/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6052 - mae: 0.9813 - val_loss: 1.6402 - val_mae: 0.9794\n",
      "Epoch 225/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5809 - mae: 0.9724 - val_loss: 1.6424 - val_mae: 0.9788\n",
      "Epoch 226/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5918 - mae: 0.9816 - val_loss: 1.6345 - val_mae: 0.9871\n",
      "Epoch 227/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5762 - mae: 0.9726 - val_loss: 1.6308 - val_mae: 0.9853\n",
      "Epoch 228/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5854 - mae: 0.9809 - val_loss: 1.6670 - val_mae: 1.0196\n",
      "Epoch 229/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5682 - mae: 0.9725 - val_loss: 1.6380 - val_mae: 0.9865\n",
      "Epoch 230/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5821 - mae: 0.9756 - val_loss: 1.6466 - val_mae: 1.0049\n",
      "Epoch 231/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5724 - mae: 0.9768 - val_loss: 1.6334 - val_mae: 0.9961\n",
      "Epoch 232/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5839 - mae: 0.9770 - val_loss: 1.6408 - val_mae: 1.0061\n",
      "Epoch 233/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5814 - mae: 0.9738 - val_loss: 1.6564 - val_mae: 1.0183\n",
      "Epoch 234/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5766 - mae: 0.9782 - val_loss: 1.6424 - val_mae: 0.9971\n",
      "Epoch 235/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5847 - mae: 0.9832 - val_loss: 1.6471 - val_mae: 0.9765\n",
      "Epoch 236/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5789 - mae: 0.9732 - val_loss: 1.6595 - val_mae: 1.0193\n",
      "Epoch 237/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5791 - mae: 0.9747 - val_loss: 1.6387 - val_mae: 0.9992\n",
      "Epoch 238/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5775 - mae: 0.9771 - val_loss: 1.6291 - val_mae: 0.9918\n",
      "Epoch 239/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6210 - mae: 0.9990 - val_loss: 1.6334 - val_mae: 0.9924\n",
      "Epoch 240/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.6038 - mae: 0.9903 - val_loss: 1.6422 - val_mae: 0.9991\n",
      "Epoch 241/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5823 - mae: 0.9758 - val_loss: 1.6375 - val_mae: 0.9970\n",
      "Epoch 242/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5813 - mae: 0.9798 - val_loss: 1.6332 - val_mae: 0.9883\n",
      "Epoch 243/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5811 - mae: 0.9780 - val_loss: 1.6334 - val_mae: 0.9892\n",
      "Epoch 244/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5822 - mae: 0.9815 - val_loss: 1.6345 - val_mae: 0.9927\n",
      "Epoch 245/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5728 - mae: 0.9787 - val_loss: 1.6328 - val_mae: 0.9876\n",
      "Epoch 246/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5858 - mae: 0.9772 - val_loss: 1.6318 - val_mae: 0.9898\n",
      "Epoch 247/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5800 - mae: 0.9811 - val_loss: 1.6336 - val_mae: 0.9898\n",
      "Epoch 248/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5824 - mae: 0.9813 - val_loss: 1.6371 - val_mae: 0.9798\n",
      "Epoch 249/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.5563 - mae: 0.9687 - val_loss: 1.6436 - val_mae: 0.9761\n",
      "Epoch 250/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5898 - mae: 0.9734 - val_loss: 1.6319 - val_mae: 0.9837\n",
      "Epoch 251/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5938 - mae: 0.9788 - val_loss: 1.6333 - val_mae: 0.9819\n",
      "Epoch 252/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5923 - mae: 0.9813 - val_loss: 1.6323 - val_mae: 0.9879\n",
      "Epoch 253/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.5683 - mae: 0.9713 - val_loss: 1.6650 - val_mae: 1.0229\n",
      "Epoch 254/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5972 - mae: 0.9883 - val_loss: 1.6324 - val_mae: 0.9936\n",
      "Epoch 255/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5574 - mae: 0.9667 - val_loss: 1.6337 - val_mae: 0.9891\n",
      "Epoch 256/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5739 - mae: 0.9738 - val_loss: 1.6397 - val_mae: 0.9812\n",
      "Epoch 257/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5778 - mae: 0.9789 - val_loss: 1.6344 - val_mae: 0.9822\n",
      "Epoch 258/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5872 - mae: 0.9754 - val_loss: 1.6506 - val_mae: 0.9768\n",
      "Epoch 259/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5874 - mae: 0.9840 - val_loss: 1.6417 - val_mae: 0.9957\n",
      "Epoch 260/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5798 - mae: 0.9809 - val_loss: 1.6426 - val_mae: 0.9756\n",
      "Epoch 261/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5752 - mae: 0.9759 - val_loss: 1.6334 - val_mae: 0.9916\n",
      "Epoch 262/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5641 - mae: 0.9691 - val_loss: 1.7100 - val_mae: 1.0486\n",
      "Epoch 263/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5873 - mae: 0.9886 - val_loss: 1.6480 - val_mae: 1.0095\n",
      "Epoch 264/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5803 - mae: 0.9733 - val_loss: 1.6344 - val_mae: 0.9862\n",
      "Epoch 265/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5923 - mae: 0.9810 - val_loss: 1.6434 - val_mae: 0.9956\n",
      "Epoch 266/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6037 - mae: 0.9837 - val_loss: 1.6335 - val_mae: 0.9933\n",
      "Epoch 267/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5503 - mae: 0.9702 - val_loss: 1.6824 - val_mae: 0.9731\n",
      "Epoch 268/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.6023 - mae: 0.9836 - val_loss: 1.6394 - val_mae: 0.9932\n",
      "Epoch 269/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5608 - mae: 0.9699 - val_loss: 1.6851 - val_mae: 1.0367\n",
      "Epoch 270/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5779 - mae: 0.9817 - val_loss: 1.6500 - val_mae: 1.0039\n",
      "Epoch 271/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5894 - mae: 0.9778 - val_loss: 1.6820 - val_mae: 1.0291\n",
      "Epoch 272/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.6017 - mae: 0.9828 - val_loss: 1.6403 - val_mae: 1.0040\n",
      "Epoch 273/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5729 - mae: 0.9743 - val_loss: 1.6858 - val_mae: 1.0338\n",
      "Epoch 274/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5793 - mae: 0.9744 - val_loss: 1.6383 - val_mae: 0.9918\n",
      "Epoch 275/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5752 - mae: 0.9798 - val_loss: 1.6405 - val_mae: 0.9951\n",
      "Epoch 276/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5842 - mae: 0.9783 - val_loss: 1.6429 - val_mae: 1.0039\n",
      "Epoch 277/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6087 - mae: 0.9825 - val_loss: 1.6421 - val_mae: 0.9882\n",
      "Epoch 278/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5804 - mae: 0.9782 - val_loss: 1.6553 - val_mae: 0.9735\n",
      "Epoch 279/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5927 - mae: 0.9845 - val_loss: 1.6364 - val_mae: 0.9878\n",
      "Epoch 280/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5763 - mae: 0.9766 - val_loss: 1.6371 - val_mae: 0.9948\n",
      "Epoch 281/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5574 - mae: 0.9692 - val_loss: 1.6486 - val_mae: 1.0008\n",
      "Epoch 282/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5766 - mae: 0.9817 - val_loss: 1.6348 - val_mae: 0.9850\n",
      "Epoch 283/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5767 - mae: 0.9695 - val_loss: 1.6767 - val_mae: 1.0287\n",
      "Epoch 284/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6004 - mae: 0.9862 - val_loss: 1.6839 - val_mae: 1.0341\n",
      "Epoch 285/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.6056 - mae: 0.9828 - val_loss: 1.6425 - val_mae: 0.9814\n",
      "Epoch 286/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6007 - mae: 0.9817 - val_loss: 1.6439 - val_mae: 0.9768\n",
      "Epoch 287/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5843 - mae: 0.9765 - val_loss: 1.6347 - val_mae: 0.9905\n",
      "Epoch 288/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5739 - mae: 0.9769 - val_loss: 1.7192 - val_mae: 1.0509\n",
      "Epoch 289/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6096 - mae: 0.9898 - val_loss: 1.6359 - val_mae: 0.9902\n",
      "Epoch 290/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5791 - mae: 0.9758 - val_loss: 1.6401 - val_mae: 0.9810\n",
      "Epoch 291/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.6095 - mae: 0.9894 - val_loss: 1.6473 - val_mae: 0.9763\n",
      "Epoch 292/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.5678 - mae: 0.9800 - val_loss: 1.6619 - val_mae: 0.9728\n",
      "Epoch 293/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5806 - mae: 0.9696 - val_loss: 1.6467 - val_mae: 1.0098\n",
      "Epoch 294/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5910 - mae: 0.9871 - val_loss: 1.6439 - val_mae: 0.9983\n",
      "Epoch 295/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5693 - mae: 0.9661 - val_loss: 1.6542 - val_mae: 1.0149\n",
      "Epoch 296/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5948 - mae: 0.9797 - val_loss: 1.6448 - val_mae: 1.0100\n",
      "Epoch 297/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5611 - mae: 0.9757 - val_loss: 1.6413 - val_mae: 1.0065\n",
      "Epoch 298/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5822 - mae: 0.9768 - val_loss: 1.6339 - val_mae: 0.9807\n",
      "Epoch 299/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5965 - mae: 0.9840 - val_loss: 1.6362 - val_mae: 0.9798\n",
      "Epoch 300/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5993 - mae: 0.9765 - val_loss: 1.6327 - val_mae: 0.9968\n",
      "Epoch 301/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5775 - mae: 0.9813 - val_loss: 1.6493 - val_mae: 0.9816\n",
      "Epoch 302/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5801 - mae: 0.9752 - val_loss: 1.6609 - val_mae: 1.0229\n",
      "Epoch 303/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5664 - mae: 0.9759 - val_loss: 1.6632 - val_mae: 1.0070\n",
      "Epoch 304/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.5702 - mae: 0.9795 - val_loss: 1.6615 - val_mae: 1.0164\n",
      "Epoch 305/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5768 - mae: 0.9719 - val_loss: 1.6347 - val_mae: 0.9894\n",
      "Epoch 306/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5757 - mae: 0.9738 - val_loss: 1.6430 - val_mae: 0.9788\n",
      "Epoch 307/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5837 - mae: 0.9681 - val_loss: 1.6373 - val_mae: 0.9864\n",
      "Epoch 308/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5961 - mae: 0.9800 - val_loss: 1.7007 - val_mae: 1.0461\n",
      "Epoch 309/3000\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.5928 - mae: 0.9762 - val_loss: 1.6442 - val_mae: 0.9800\n",
      "Epoch 310/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5894 - mae: 0.9833 - val_loss: 1.6463 - val_mae: 1.0024\n",
      "Epoch 311/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5809 - mae: 0.9760 - val_loss: 1.6385 - val_mae: 0.9772\n",
      "Epoch 312/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5940 - mae: 0.9840 - val_loss: 1.6356 - val_mae: 0.9807\n",
      "Epoch 313/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5998 - mae: 0.9817 - val_loss: 1.6491 - val_mae: 1.0137\n",
      "Epoch 314/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5806 - mae: 0.9749 - val_loss: 1.6368 - val_mae: 0.9809\n",
      "Epoch 315/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5841 - mae: 0.9844 - val_loss: 1.6320 - val_mae: 0.9877\n",
      "Epoch 316/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5697 - mae: 0.9765 - val_loss: 1.6381 - val_mae: 0.9992\n",
      "Epoch 317/3000\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5779 - mae: 0.9805 - val_loss: 1.6495 - val_mae: 1.0098\n",
      "Epoch 318/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5695 - mae: 0.9759 - val_loss: 1.6299 - val_mae: 0.9901\n",
      "Epoch 319/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5968 - mae: 0.9840 - val_loss: 1.6309 - val_mae: 0.9846\n",
      "Epoch 320/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5636 - mae: 0.9644 - val_loss: 1.6556 - val_mae: 1.0174\n",
      "Epoch 321/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.6097 - mae: 0.9862 - val_loss: 1.6377 - val_mae: 0.9883\n",
      "Epoch 322/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5860 - mae: 0.9827 - val_loss: 1.6372 - val_mae: 0.9957\n",
      "Epoch 323/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5655 - mae: 0.9746 - val_loss: 1.6421 - val_mae: 1.0016\n",
      "Epoch 324/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5742 - mae: 0.9743 - val_loss: 1.6465 - val_mae: 1.0108\n",
      "Epoch 325/3000\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 1.5856 - mae: 0.9781 - val_loss: 1.6427 - val_mae: 0.9788\n",
      "Epoch 326/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5816 - mae: 0.9811 - val_loss: 1.6918 - val_mae: 0.9721\n",
      "Epoch 327/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6120 - mae: 0.9836 - val_loss: 1.6657 - val_mae: 0.9724\n",
      "Epoch 328/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5914 - mae: 0.9855 - val_loss: 1.6419 - val_mae: 0.9817\n",
      "Epoch 329/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5861 - mae: 0.9734 - val_loss: 1.6369 - val_mae: 0.9837\n",
      "Epoch 330/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5794 - mae: 0.9780 - val_loss: 1.6633 - val_mae: 1.0196\n",
      "Epoch 331/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.6127 - mae: 0.9858 - val_loss: 1.6355 - val_mae: 0.9908\n",
      "Epoch 332/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5696 - mae: 0.9763 - val_loss: 1.6397 - val_mae: 0.9810\n",
      "Epoch 333/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5990 - mae: 0.9794 - val_loss: 1.6446 - val_mae: 1.0032\n",
      "Epoch 334/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5791 - mae: 0.9738 - val_loss: 1.6632 - val_mae: 1.0232\n",
      "Epoch 335/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6115 - mae: 0.9948 - val_loss: 1.6546 - val_mae: 1.0036\n",
      "Epoch 336/3000\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.5810 - mae: 0.9784 - val_loss: 1.6349 - val_mae: 0.9933\n",
      "Epoch 337/3000\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5882 - mae: 0.9808 - val_loss: 1.6735 - val_mae: 1.0249\n",
      "Epoch 338/3000\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5547 - mae: 0.9685 - val_loss: 1.6345 - val_mae: 0.9829\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :\n",
      "\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.6291 - mae: 0.9918\n",
      "11/11 [==============================] - 0s 4ms/step\n",
      " :\n",
      "Test Loss (MSE): 1.6291379928588867\n",
      "Test MAE: 0.9917668700218201\n",
      "MIN: [0.64804465]\n",
      "MAX: [2.5576627]\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.4060 - mae: 0.9441\n",
      "11/11 [==============================] - 0s 4ms/step\n",
      " :\n",
      "Test Loss (MSE): 1.406014084815979\n",
      "Test MAE: 0.9441463947296143\n",
      "MIN: [0.4899825]\n",
      "MAX: [2.0512094]\n",
      "\n",
      " :\n",
      "\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.4299 - mae: 0.9282\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      " :\n",
      "Test Loss (MSE): 1.4298955202102661\n",
      "Test MAE: 0.9282474517822266\n",
      "MIN: [0.7350475]\n",
      "MAX: [2.809929]\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.9401 - mae: 0.7456\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      " :\n",
      "Test Loss (MSE): 0.9401243925094604\n",
      "Test MAE: 0.745553731918335\n",
      "MIN: [0.57837015]\n",
      "MAX: [1.8055099]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbol\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "train_model(x_train_home, y_train_home, x_train_away, \n",
    "                       y_train_away, x_valid_home, y_valid_home, x_valid_away, y_valid_away, x_test_home, y_test_home, x_test_away, y_test_away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n     :\\n         - MSE: 1.239810  MAE: 0.908011.   StandardScaller, Ridge(alpha=1.0, max_iter=100000)\\n    \\n         - MSE: 1.260653  MAE: 0.906175.   PolynomialFeatures(degree=2, include_bias=False), Ridge(alpha=1.0)\\n\\n         - MSE: 1.246023  MAE: 0.908439.   max_depth=2, min_samples_leaf=12, n_estimators=300, min_samples_split=3\\n\\n     :\\n      - MSE: 1.32321703  MAE: 0.903338.  MinMaxScaler      \\n    \\n     :\\n         - MSE: 1.198250  MAE: 0.898801.\\n    \\n         - MSE: 1.220643  MAE: 0.904297.\\n\\n         - MSE: 1.187938  MAE: 0.897064.\\n\\n      - MSE: 1.248805  MAE: 0.904949.\\n\\n'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "     :\n",
    "         - MSE: 1.482851  MAE: 0.973542.   StandardScaller, Ridge(alpha=1.0, max_iter=100000)\n",
    "    \n",
    "         - MSE: 1.499869  MAE: 0.977403.   PolynomialFeatures(degree=2, include_bias=False), Ridge(alpha=1.0)\n",
    "\n",
    "         - MSE: 1.473911  MAE: 0.959795.   max_depth=2, min_samples_leaf=12, n_estimators=300, min_samples_split=3\n",
    "\n",
    "     :\n",
    "      - MSE: 1.517575995  MAE: 0.96661535.  MinMaxScaler      \n",
    "    \n",
    "     :\n",
    "         - MSE: 1.160214  MAE: 0.843700.\n",
    "    \n",
    "         - MSE: 1.157167  MAE: 0.837402\n",
    "\n",
    "         - MSE: 1.140186  MAE: 0.821686.\n",
    "\n",
    "      - MSE: 1.185009955  MAE: 0.836900.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 0, 4, 3, 2, 4, 2, 0, 0, 2, 5, 2, 0, 2, 4, 3, 0, 5, 0, 0, 5,\n",
       "       1, 1, 2, 2, 0, 3, 4, 2, 1, 2, 4, 4, 0, 3, 2, 2, 0, 4, 0, 1, 0, 2,\n",
       "       2, 2, 4, 0, 4, 2, 1, 3, 1, 0, 0, 3, 1, 1, 3, 1, 2, 1, 2, 0, 4, 1,\n",
       "       3, 3, 3, 0, 3, 3, 4, 1, 0, 1, 4, 2, 0, 0, 4, 5, 3, 4, 1, 0, 0, 3,\n",
       "       0, 5, 3, 1, 5, 3, 5, 5, 3, 0, 3, 0, 2, 4, 0, 0, 3, 0, 0, 0, 0, 2,\n",
       "       4, 1, 4, 5, 2, 3, 2, 5, 1, 1, 2, 3, 2, 1, 2, 5, 2, 1, 3, 2, 2, 4,\n",
       "       1, 5, 4, 1, 3, 5, 1, 0, 2, 5, 4, 5, 1, 3, 0, 2, 0, 5, 1, 0, 1, 0,\n",
       "       2, 3, 4, 5, 4, 4, 5, 2, 5, 3, 4, 5, 5, 3, 4, 3, 1, 0, 2, 0, 2, 0,\n",
       "       3, 0, 2, 1, 4, 5, 0, 3, 5, 2, 4, 1, 1, 5, 1, 2, 5, 0, 2, 0, 2, 0,\n",
       "       1, 2, 1, 3, 2, 2, 0, 1, 5, 2, 1, 0, 3, 4, 5, 4, 3, 4, 1, 0, 5, 5,\n",
       "       3, 0, 1, 0, 5, 3, 0, 4, 5, 2, 3, 3, 2, 1, 0, 5, 0, 1, 0, 2, 5, 4,\n",
       "       0, 5, 4, 0, 1, 4, 3, 5, 4, 4, 0, 2, 4, 0, 0, 4, 4, 1, 1, 2, 0, 2,\n",
       "       1, 1, 4, 2, 3, 5, 0, 1, 1, 5, 3, 4, 3, 0, 4, 2, 5, 1, 2, 5, 2, 0,\n",
       "       2, 2, 4, 5, 1, 3, 3, 2, 3, 3, 1, 2, 1, 4, 1, 3, 1, 2, 5, 3, 4, 4,\n",
       "       2, 1, 1, 1, 3, 5, 2, 0, 1, 5, 0, 3, 5, 1, 0, 1, 4, 2, 4, 0, 5, 5,\n",
       "       3, 1, 0, 1, 3, 2, 0, 4, 3, 2])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "      \n",
    "'''\n",
    "y_pred_home_random = np.random.randint(6, size=len(y_valid_home))\n",
    "y_pred_away_random = np.random.randint(6, size=len(y_valid_away))\n",
    "y_pred_home_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE home: 4.811764705882353\n",
      "MAE home: 1.7529411764705882\n",
      "MSE away: 6.279411764705882\n",
      "MAE away: 2.0441176470588234\n"
     ]
    }
   ],
   "source": [
    "mse_random_home = mean_squared_error(y_valid_home, y_pred_home_random)\n",
    "mae_random_home = mean_absolute_error(y_valid_home, y_pred_home_random)\n",
    "\n",
    "mse_random_away = mean_squared_error(y_valid_away, y_pred_away_random)\n",
    "mae_random_away = mean_absolute_error(y_valid_away, y_pred_away_random)\n",
    "\n",
    "print(f'MSE home: {mse_random_home}')\n",
    "print(f'MAE home: {mae_random_home}')\n",
    "\n",
    "print(f'MSE away: {mse_random_away}')\n",
    "print(f'MAE away: {mae_random_away}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 5, 1, 0, 0, 0, 3, 0, 2, 1, 2, 0, 0, 1, 6, 3, 3, 1, 1, 0, 0, 1,\n",
       "       1, 2, 2, 2, 2, 3, 2, 1, 1, 4, 1, 0, 3, 1, 0, 1, 3, 0, 1, 2, 0, 1,\n",
       "       1, 1, 1, 2, 2, 0, 2, 1, 1, 2, 1, 0, 1, 1, 0, 2, 3, 2, 1, 3, 1, 2,\n",
       "       0, 2, 1, 0, 2, 1, 1, 2, 2, 1, 0, 2, 3, 0, 1, 0, 0, 1, 0, 1, 2, 2,\n",
       "       0, 2, 2, 5, 2, 4, 1, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 0, 0, 1,\n",
       "       0, 1, 2, 2, 0, 0, 1, 2, 0, 2, 1, 2, 1, 2, 1, 1, 1, 3, 1, 2, 0, 0,\n",
       "       0, 2, 2, 1, 1, 0, 0, 2, 0, 3, 1, 1, 2, 1, 0, 2, 2, 1, 2, 3, 4, 1,\n",
       "       0, 3, 1, 1, 5, 0, 1, 2, 1, 1, 2, 2, 4, 2, 0, 0, 1, 1, 1, 2, 0, 2,\n",
       "       1, 2, 1, 1, 0, 2, 1, 4, 2, 0, 1, 2, 0, 2, 2, 4, 0, 2, 3, 1, 1, 0,\n",
       "       4, 2, 1, 0, 1, 1, 1, 2, 2, 1, 3, 2, 1, 2, 1, 4, 3, 1, 1, 0, 3, 1,\n",
       "       2, 2, 3, 2, 1, 0, 2, 0, 3, 3, 2, 2, 3, 1, 0, 1, 4, 1, 1, 0, 2, 3,\n",
       "       3, 1, 1, 2, 4, 0, 0, 1, 1, 2, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 3, 2, 4, 1, 2, 0, 0, 1, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 1,\n",
       "       3, 2, 2, 2, 0, 0, 1, 1, 3, 1, 0, 2, 1, 2, 0, 1, 1, 1, 1, 0, 1, 2,\n",
       "       0, 2, 2, 5, 2, 2, 1, 0, 1, 1, 2, 0, 4, 2, 3, 0, 2, 4, 2, 0, 2, 0,\n",
       "       3, 1, 0, 2, 0, 1, 0, 3, 1, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "          y_train\n",
    "'''\n",
    "\n",
    "probabilities_home = [np.sum(y_train_home == 0) / len(y_train_home), \n",
    "                      np.sum(y_train_home == 1) / len(y_train_home), \n",
    "                      np.sum(y_train_home == 2) / len(y_train_home), \n",
    "                      np.sum(y_train_home == 3) / len(y_train_home), \n",
    "                      np.sum(y_train_home == 4) / len(y_train_home),\n",
    "                      np.sum(y_train_home == 5) / len(y_train_home),\n",
    "                      np.sum(y_train_home == 6) / len(y_train_home)]\n",
    "probabilities_away = [np.sum(y_train_away == 0) / len(y_train_away), \n",
    "                      np.sum(y_train_away == 1) / len(y_train_away), \n",
    "                      np.sum(y_train_away == 2) / len(y_train_away), \n",
    "                      np.sum(y_train_away == 3) / len(y_train_away), \n",
    "                      np.sum(y_train_away == 4) / len(y_train_away),\n",
    "                      np.sum(y_train_away == 5) / len(y_train_away),\n",
    "                      np.sum(y_train_away == 6) / len(y_train_away)]\n",
    "\n",
    "y_pred_home_prob = np.random.choice([0, 1, 2, 3, 4, 5, 6], size=len(y_valid_home), p=probabilities_home)\n",
    "y_pred_away_prob = np.random.choice([0, 1, 2, 3, 4, 5, 6], size=len(y_valid_away), p=probabilities_away)\n",
    "y_pred_away_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE home: 3.5676470588235296\n",
      "MAE home: 1.45\n",
      "MSE away: 2.6411764705882352\n",
      "MAE away: 1.2352941176470589\n"
     ]
    }
   ],
   "source": [
    "mse_prob_home = mean_squared_error(y_valid_home, y_pred_home_prob)\n",
    "mae_prob_home = mean_absolute_error(y_valid_home, y_pred_home_prob)\n",
    "\n",
    "mse_prob_away = mean_squared_error(y_valid_away, y_pred_away_prob)\n",
    "mae_prob_away = mean_absolute_error(y_valid_away, y_pred_away_prob)\n",
    "\n",
    "print(f'MSE home: {mse_prob_home}')\n",
    "print(f'MAE home: {mae_prob_home}')\n",
    "\n",
    "print(f'MSE away: {mse_prob_away}')\n",
    "print(f'MAE away: {mae_prob_away}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
